{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "Compute_base_performances.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmanuelAlogna/MRI-generation/blob/master/src/Compute_base_similarities.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gfvAKewMUne",
        "colab_type": "text"
      },
      "source": [
        "## Import libraries and mount the drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngnfgyxdgvDv",
        "colab_type": "code",
        "outputId": "0b2fcb07-fb1d-47b3-faba-1bd4063b4f94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from IPython import display\n",
        "import time\n",
        "import math\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n",
            "2.1.0\n",
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUnILm4yPhpC",
        "colab_type": "text"
      },
      "source": [
        "## Loading the validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1NF4LOngPNi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_feature_description(modalities):                                                   \n",
        "                                                                                           \n",
        "    feature_description =lambda mod : {                                                    \n",
        "                            mod+'_mri': tf.io.FixedLenFeature([], tf.string),              \n",
        "                            mod+'_path': tf.io.FixedLenFeature([], tf.string),             \n",
        "                                                                                           \n",
        "                            mod+'_mri_min': tf.io.FixedLenFeature([], tf.float32),         \n",
        "                            mod+'_mri_min_src': tf.io.FixedLenFeature([], tf.float32),     \n",
        "                            mod+'_mri_max': tf.io.FixedLenFeature([], tf.float32),         \n",
        "                            mod+'_mri_max_src': tf.io.FixedLenFeature([], tf.float32),     \n",
        "                                                                                           \n",
        "                            mod+'_mri_lperc': tf.io.FixedLenFeature([], tf.float32),       \n",
        "                            mod+'_mri_hperc': tf.io.FixedLenFeature([], tf.float32),       \n",
        "                            mod+'_mri_hperc_src': tf.io.FixedLenFeature([], tf.float32),   \n",
        "                            mod+'_mri_lperc_src': tf.io.FixedLenFeature([], tf.float32),   \n",
        "                                                                                           \n",
        "                            mod+'_x_dimension': tf.io.FixedLenFeature([], tf.int64),       \n",
        "                            mod+'_y_dimension': tf.io.FixedLenFeature([], tf.int64),       \n",
        "                            mod+'_z_dimension': tf.io.FixedLenFeature([], tf.int64),       \n",
        "                            mod+'_z_dimension_src': tf.io.FixedLenFeature([], tf.int64),   \n",
        "                            mod+'_x_dimension_src': tf.io.FixedLenFeature([], tf.int64),   \n",
        "                            mod+'_y_dimension_src': tf.io.FixedLenFeature([], tf.int64),   \n",
        "                                                                                           \n",
        "                            mod+'_x_origin_src': tf.io.FixedLenFeature([], tf.float32),    \n",
        "                            mod+'_y_origin_src': tf.io.FixedLenFeature([], tf.float32),    \n",
        "                            mod+'_z_origin_src': tf.io.FixedLenFeature([], tf.float32),    \n",
        "                                                                                           \n",
        "                            mod+'_z_spacing_src': tf.io.FixedLenFeature([], tf.float32),   \n",
        "                            mod+'_x_spacing_src': tf.io.FixedLenFeature([], tf.float32),   \n",
        "                            mod+'_y_spacing_src': tf.io.FixedLenFeature([], tf.float32),   \n",
        "                                                                                           \n",
        "                            mod+'_patient': tf.io.FixedLenFeature([], tf.string),          \n",
        "                            mod+'_sample_number': tf.io.FixedLenFeature([], tf.string),    \n",
        "                            mod+'_patient_grade': tf.io.FixedLenFeature([], tf.string),    \n",
        "                            mod+'_location': tf.io.FixedLenFeature([], tf.string),         \n",
        "                            mod+'_dataset_version': tf.io.FixedLenFeature([], tf.string),  \n",
        "                            mod+'_dataset_name': tf.io.FixedLenFeature([], tf.string),     \n",
        "                            mod+'_mri_type': tf.io.FixedLenFeature([], tf.string),         \n",
        "                            mod+'_dataset_split': tf.io.FixedLenFeature([], tf.string),    \n",
        "                            mod+'_patient_mri_seq': tf.io.FixedLenFeature([], tf.string),  \n",
        "                          }                                                                \n",
        "    features = {}                                                                          \n",
        "    for mod in modalities:\n",
        "        features.update(feature_description(mod))\n",
        "    return features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvRByzzuUfst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 32"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hScO6uddgPNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_gzip_compression = True\n",
        "\n",
        "def load_dataset(name, mri_type, center_crop=None, random_crop=None, filter=None, batch_size=BATCH_SIZE, cache=True, \n",
        "                 prefetch_buffer=1, shuffle_buffer=128, interleave=1, cast_to=tf.float32, clip_labels_to=0.0, \n",
        "                 take_only=None, shuffle=True, infinite=False, n_threads=os.cpu_count()):\n",
        "    def parse_sample(sample_proto):\n",
        "        parsed = tf.io.parse_single_example(sample_proto, get_feature_description([\"OT\"]+mri_type))\n",
        "        # Decoding image arrays\n",
        "        \n",
        "        slice_shape = [parsed['OT_x_dimension'.format(mri_type[0])], parsed['OT_y_dimension'], 1]\n",
        "        # Decoding the ground truth\n",
        "        parsed['seg'] = tf.cast(tf.reshape(tf.io.decode_raw(parsed['OT_mri'], tf.float32), shape=slice_shape), dtype=cast_to)\n",
        "        # Decode each channel and stack in a 3d volume\n",
        "        stacked_mri = list()\n",
        "        for mod in mri_type:\n",
        "            stacked_mri.append(tf.cast(tf.reshape(tf.io.decode_raw(parsed['{}_mri'.format(mod)], tf.float32), shape=slice_shape), dtype=cast_to))\n",
        "        parsed['mri'] = tf.concat(stacked_mri, axis=-1)\n",
        "        # Clipping the labels if requested\n",
        "        parsed['seg'] = tf.clip_by_value(parsed['seg'], 0.0, clip_labels_to) if clip_labels_to else parsed['seg']\n",
        "        \n",
        "        # Cropping\n",
        "        if random_crop or center_crop:\n",
        "            # Stacking the mri and the label to align the crop shape\n",
        "            mri_seg = tf.concat([parsed['mri'], parsed['seg']], axis=-1)\n",
        "            if random_crop:\n",
        "                random_crop[-1] = mri_seg.shape[-1] \n",
        "                cropped = tf.image.random_crop(mri_seg, size=random_crop)\n",
        "            else:\n",
        "                cropped = tf.image.resize_with_crop_or_pad(mri_seg,center_crop[0],center_crop[1])\n",
        "            # Splitting back\n",
        "            parsed['mri'] = cropped[:,:,:len(mri_type)]\n",
        "            parsed['seg'] = cropped[:,:,len(mri_type):]\n",
        "        \n",
        "        return parsed\n",
        "    \n",
        "    path = './drive/My Drive/MRI-generation/{}.tfrecords'.format(name)\n",
        "    dataset = tf.data.TFRecordDataset(path, compression_type='GZIP' if use_gzip_compression else \"\")\n",
        "    dataset = dataset.filter(filter) if filter is not None else dataset\n",
        "    dataset = dataset.take(take_only) if take_only is not None else dataset\n",
        "\n",
        "    # You should generally cache after loading and preprocessing the data, \n",
        "    # but before shuffling, repeating, batching and prefetching”\n",
        "    dataset = dataset.cache() if cache else dataset\n",
        "    if shuffle and infinite:\n",
        "        dataset = dataset.apply(tf.data.experimental.shuffle_and_repeat(shuffle_buffer))\n",
        "    else:\n",
        "        dataset = dataset.shuffle(shuffle_buffer, reshuffle_each_iteration=True) if shuffle else dataset\n",
        "        dataset = dataset.repeat() if infinite else dataset\n",
        "    dataset = dataset.map(parse_sample, num_parallel_calls=None)\n",
        "    dataset = dataset.batch(batch_size) if batch_size > 0 else dataset\n",
        "\n",
        "    if interleave > 1:\n",
        "        dataset = dataset.interleave(lambda x: tf.data.Dataset.from_tensors(x).repeat(interleave), cycle_length=n_threads, block_length=interleave, num_parallel_calls=n_threads)\n",
        "  \n",
        "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBbtf3_Rlcsr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading only validation and testing (no need to load the training)\n",
        "\n",
        "def load_datasets():\n",
        "    validation = load_dataset('brats2015_validation_crop_mri', ['MR_T1', 'MR_T1c', 'MR_T2', 'MR_Flair'], shuffle=False)\n",
        "    training = load_dataset('brats2015_training_crop_mri', ['MR_T1', 'MR_T1c', 'MR_T2', 'MR_Flair'], shuffle=True)\n",
        "    testing = load_dataset('brats2015_testing_crop_mri', ['MR_T1', 'MR_T1c', 'MR_T2', 'MR_Flair'], shuffle=False)\n",
        "    return training, validation , testing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5la9-anCd8-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "training, validation, testing = load_datasets() # I'll use this in case I get stuck with the cache.\n",
        "\n",
        "modalities = ['MR_T1_mri', 'MR_T2_mri', 'MR_T1c_mri', 'MR_Flair_mri', 'OT_mri']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6XO7N2X2LJcM",
        "colab": {}
      },
      "source": [
        "# PROCESSING IMAGES IN THE BATCH\n",
        "def process_batch(batch):   # takes in input a raw_record[mod].numpy()\n",
        "    batch = tf.io.decode_raw(batch, tf.float32)\n",
        "    batch = tf.reshape(batch, (BATCH_SIZE, 180, 180))\n",
        "    paddings = tf.constant([[0, 0], [38, 38], [38, 38]])\n",
        "    batch = tf.pad(batch, paddings, \"CONSTANT\")\n",
        "    batch = tf.expand_dims(batch, axis=3)\n",
        "    return batch\n",
        "    \n",
        "# final shape will be (bs, 256, 256, 1): now the batch is ready to be fed to the GAN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpKFZdwoiqRU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this function preprocesses the input image and the ground truth from a raw_record\n",
        "# images in the return are then ready to be fed into the GAN\n",
        "def take_images_from_raw(raw_record, ot=False):\n",
        "\n",
        "    t1, t2, t1c, tflair = raw_record[modalities[0]], raw_record[modalities[1]], raw_record[modalities[2]], raw_record[modalities[3]]\n",
        "    t1 = process_batch(t1)\n",
        "    t2 = process_batch(t2)\n",
        "    t1c = process_batch(t1c)\n",
        "    tflair = process_batch(tflair)\n",
        "    if ot:      # if ot is true, I'll retrieve also the segmentation from the raw_record\n",
        "        segmentation = process_batch(raw_record[modalities[4]])\n",
        "        return t1, t2, t1c, tflair, segmentation\n",
        "    return t1, t2, t1c, tflair"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxXd5khuN_Us",
        "colab_type": "text"
      },
      "source": [
        "## Load some useful functions\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDybqCIjhEtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resize_with_crop(*args): # possibile arguments: input, gt, prediction and maybe the segmentation\n",
        "\n",
        "    image0 = tf.image.resize_with_crop_or_pad(args[0], 155, 194)\n",
        "    image1 = tf.image.resize_with_crop_or_pad(args[1], 155, 194)\n",
        "    image2 = tf.image.resize_with_crop_or_pad(args[2], 155, 194)\n",
        "    image3 = tf.image.resize_with_crop_or_pad(args[3], 155, 194)\n",
        "    if len(args) == 5:      # crop also the segmentation, if is given as additional argument\n",
        "        image4 = tf.image.resize_with_crop_or_pad(args[4], 155, 194)\n",
        "        return image0, image1, image2, image3, image4\n",
        "    if len(args) == 7:      # crop also the segmentation, if is given as additional argument\n",
        "        image4 = tf.image.resize_with_crop_or_pad(args[4], 155, 194)\n",
        "        image5 = tf.image.resize_with_crop_or_pad(args[5], 155, 194)\n",
        "        image6 = tf.image.resize_with_crop_or_pad(args[6], 155, 194)\n",
        "        return image0, image1, image2, image3, image4, image5, image6\n",
        "    if len(args) == 8:      # crop also the segmentation, if is given as additional argument\n",
        "        image4 = tf.image.resize_with_crop_or_pad(args[4], 155, 194)\n",
        "        image5 = tf.image.resize_with_crop_or_pad(args[5], 155, 194)\n",
        "        image6 = tf.image.resize_with_crop_or_pad(args[6], 155, 194)\n",
        "        image7 = tf.image.resize_with_crop_or_pad(args[7], 155, 194)\n",
        "        return image0, image1, image2, image3, image4, image5, image6, image7\n",
        "    return image0, image1, image2, image3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0DtPOglvkn_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MASK THE SEGMENTATION with the prediciton and the ground_truth and then compute the metrics (even if there is the black pixels\n",
        "# it should give me an idea about how much is going the generation of the tumor during the generation. of course the score\n",
        "# will be influeced (biased) by all the black pixels.)\n",
        "\n",
        "def retrieve_tumor_area(*args):  # ground_truth, prediction1, prediction2, segmentation, prediction3*\n",
        "    prediction1_np = args[0].numpy()\n",
        "    prediction2_np = args[1].numpy()\n",
        "    segmentation_np = args[2].numpy()\n",
        "    \n",
        "    idx = (segmentation_np==0)      \n",
        "    prediction1_np[idx] = segmentation_np[idx]\n",
        "    prediction2_np[idx] = segmentation_np[idx]\n",
        "    \n",
        "    return prediction1_np, prediction2_np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlUe7ol7uUab",
        "colab_type": "text"
      },
      "source": [
        "## Mean normalization\n",
        "\n",
        "![alt text](https://wikimedia.org/api/rest_v1/media/math/render/svg/5c591a0eeba163a12f69f937adbae5886d6273db)\n",
        "\n",
        "In the paper they say: \"Each patient scan is normalized by dividing each sequence by its mean intensity value. \"\n",
        "But the formula is taken from a lecture from Andrew Ng, where he defines the Mean normalization as in the formula above. (resource: https://www.youtube.com/watch?v=e1nTgoDI_m8)\n",
        "\n",
        "See also: https://stats.stackexchange.com/questions/138046/normalizations-dividing-by-mean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4LLqv4puYxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mean_rescale(x, xmin, xmax):\n",
        "    mean = tf.reduce_mean(x)\n",
        "    return ((x-mean)/(xmax-xmin)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9I9cJ9_iubPK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def mean_normalize(image):\n",
        "    image_normalized = tf.TensorArray(tf.float32, size=BATCH_SIZE)\n",
        "    for i in range(BATCH_SIZE):\n",
        "        # rescaling each image in the batch\n",
        "        max_value = tf.math.reduce_max(image[i])\n",
        "        min_value = tf.math.reduce_min(image[i])\n",
        "        x = mean_rescale(image[i], min_value, max_value)\n",
        "        image_normalized = image_normalized.write(i, x)\n",
        "    image_normalized = image_normalized.stack()\n",
        "    return image_normalized"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CexZ7tWcyRuz",
        "colab_type": "text"
      },
      "source": [
        "## Discard black images from batch (put values to 'nan')\n",
        "\n",
        "This normalization is just to test the metrics and see if there is a big difference in normalizing the prediction and the gt.\n",
        "This method is used to normalize (and so put to 'nan') only the black images, while the other images of the batch are kept with the original values. This allows me to discard the black images in the computation of the metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3RkK4z0ug06",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this normalization is just to test the metrics and see if there is a big difference in normalizing the prediction and the gt.\n",
        "# this method is used to normalize (and so put to 'nan') only the black images, while the other images of the batch are kept \n",
        "# with the original values. This allows me to discard the black images in the computation of the metrics.\n",
        "\n",
        "def black_images_to_nan(image): \n",
        "    image_normalized = tf.TensorArray(tf.float32, size=BATCH_SIZE)\n",
        "    for i in range(BATCH_SIZE):\n",
        "        # rescaling each image in the batch\n",
        "        max_value = tf.math.reduce_max(image[i])\n",
        "        min_value = tf.math.reduce_min(image[i])\n",
        "\n",
        "        # if the max = min most likely it's a black image (or an image without any important information)\n",
        "        if tf.math.equal(max_value, min_value):        \n",
        "            print(\"I found a black image! at index {}\".format(i))\n",
        "            x = mean_normalize(image[i], min_value, max_value)\n",
        "            image_normalized = image_normalized.write(i, x)\n",
        "        else:\n",
        "            image_normalized = image_normalized.write(i, image[i])\n",
        "    image_normalized = image_normalized.stack()\n",
        "    return image_normalized"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIl04K0PMurE",
        "colab_type": "text"
      },
      "source": [
        "##Defining all the quantitative metrics (PSNR, SSIM and MSE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUtrXxJe3Xvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_psnr(image1, image2):\n",
        "    # Compute PSNR over tf.float32 Tensors.\n",
        "    im1 = tf.image.convert_image_dtype(image1, tf.float32)   # inp is a numpy.ndarray and im1 is an EagerTensor\n",
        "    im2 = tf.image.convert_image_dtype(image2, tf.float32)\n",
        "    psnr = tf.image.psnr(im1, im2, max_val=1.0)\n",
        "    mean = tf.reduce_mean(tf.boolean_mask((psnr), tf.math.is_finite(psnr)))\n",
        "    std = tf.math.reduce_std(tf.boolean_mask((psnr), tf.math.is_finite(psnr)))\n",
        "    # In the computation of mean and std, I'm ignoring the 'nan' and 'inf' values\n",
        "    # Why 'nan' values? 'nan' happens when there is an image with max_value and min_value = 0.0 so a black image\n",
        "    # the PSNR would be inf (image is totally similar to the ground truth)\n",
        "    # be rescaling the image, the max_value and min_value would become nan and so the PSNR\n",
        "\n",
        "    # It ignores also the 'inf' values, in the case I don't want to normalize\n",
        "    \n",
        "    return mean, std, psnr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0kHZvJ6LhG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_ssim(image1, image2):\n",
        "    im1 = tf.image.convert_image_dtype(image1, tf.float32)   # inp is a numpy.ndarray and im1 is an EagerTensor\n",
        "    im2 = tf.image.convert_image_dtype(image2, tf.float32)\n",
        "    ssim = tf.image.ssim(im1, im2, max_val=1)\n",
        "    mean = tf.reduce_mean(tf.boolean_mask((ssim), tf.math.is_finite(ssim)))\n",
        "    std = tf.math.reduce_std(tf.boolean_mask((ssim), tf.math.is_finite(ssim)))\n",
        "    return mean, std, ssim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_kkj3HwPtK5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# I want to compute, first thing, the MSE between ground truth and generated one. The tf.function gives me a Tensor 32x256x256:\n",
        "# MSE is computed PIXEL per PIXEL, so per each of the 32 matrix 256x256, I average (1) the values of the 256x256 pixels obtaining\n",
        "# an array of 32 elements, containing the MSEs of each image belonging to the batch. Then I can average (2) these 32 to have \n",
        "# I should not average the whole 32x256x256 in one step. The result would have same mean but slightly different std.\n",
        "# I want first to obtain the MSE of each image... then I average across the batch only to have smth more accurate.\n",
        "\n",
        "def compute_mse(image1, image2):       # mean squared error\n",
        "    im1 = tf.image.convert_image_dtype(image1, tf.float32)   # inp is a numpy.ndarray and im1 is an EagerTensor\n",
        "    im2 = tf.image.convert_image_dtype(image2, tf.float32)\n",
        "    mse = tf.metrics.mean_squared_error(im1,im2)\n",
        "    # In this way is possible to do Variable item-assignment with tensors \n",
        "    mse_per_image = tf.TensorArray(tf.float32, size=BATCH_SIZE)\n",
        "    for i in range(BATCH_SIZE):\n",
        "        x = tf.reduce_mean(tf.boolean_mask((mse[i]), tf.math.is_finite(mse[i]))) \n",
        "        mse_per_image = mse_per_image.write(i, x)\n",
        "    mse_per_image = mse_per_image.stack()\n",
        "    mean = tf.reduce_mean(tf.boolean_mask((mse_per_image), tf.math.is_finite(mse_per_image)))\n",
        "    std = tf.math.reduce_std(tf.boolean_mask((mse_per_image), tf.math.is_finite(mse_per_image)))\n",
        "    return mean, std, mse_per_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEPv91vBKnHb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q  --no-deps tensorflow-addons~=0.6\n",
        "import tensorflow_addons as tfa"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xz0n_kFYas5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QCBaM8SYlsw",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-CzmRXiYmCi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_GAN(dataset, set_type, first_sequence, second_sequence, evaluate_tumor_area=False):\n",
        "    num_batches = 0\n",
        "    if set_type == 'test':\n",
        "        num_batches = 112\n",
        "    elif set_type == 'validation':\n",
        "        num_batches = 108\n",
        "    elif set_type == 'train':\n",
        "        num_batches = 876\n",
        "    \n",
        "    #num_batches = 10\n",
        "    container_psnr = tf.TensorArray(tf.float32, size=num_batches)     \n",
        "    container_mse = tf.TensorArray(tf.float32, size=num_batches)\n",
        "    container_ssim = tf.TensorArray(tf.float32, size=num_batches)\n",
        "    \n",
        "    if evaluate_tumor_area:\n",
        "        container_psnr_tumor = tf.TensorArray(tf.float32, size=num_batches)     \n",
        "        container_mse_tumor = tf.TensorArray(tf.float32, size=num_batches)\n",
        "        container_ssim_tumor = tf.TensorArray(tf.float32, size=num_batches)\n",
        "    \n",
        "    #idx = 0\n",
        "    for idx, raw_record in dataset.enumerate():\n",
        "    #for raw_record in dataset.take(10):\n",
        "        t1, t2, t1c, tflair, segmentation = take_images_from_raw(raw_record, ot=True)\n",
        "\n",
        "        # crop the images\n",
        "        t1_cr, t1c_cr, t2_cr, tflair_cr, segmentation_cr = resize_with_crop(t1, t1c, t2, tflair, segmentation)\n",
        "        \n",
        "        if first_sequence == 0:\n",
        "            prediction_normalized_cr = mean_normalize(t1_cr)\n",
        "        elif first_sequence == 1:\n",
        "            prediction_normalized_cr = mean_normalize(t2_cr)\n",
        "        elif first_sequence == 2:\n",
        "            prediction_normalized_cr = mean_normalize(t1c_cr)\n",
        "        elif first_sequence == 3:\n",
        "            prediction_normalized_cr = mean_normalize(tflair_cr)\n",
        "\n",
        "        if second_sequence == 0:\n",
        "            ground_truth_normalized_cr = mean_normalize(t1_cr)\n",
        "        elif second_sequence == 1:\n",
        "            ground_truth_normalized_cr = mean_normalize(t2_cr)\n",
        "        elif second_sequence == 2:\n",
        "            ground_truth_normalized_cr = mean_normalize(t1c_cr)\n",
        "        elif second_sequence == 3:\n",
        "            ground_truth_normalized_cr = mean_normalize(tflair_cr)\n",
        "\n",
        "        # compute the metrics of similarity\n",
        "        mean, std, psnr = compute_psnr(ground_truth_normalized_cr, prediction_normalized_cr)\n",
        "        container_psnr = container_psnr.write(idx, psnr)\n",
        "        mean, std, mse = compute_mse(ground_truth_normalized_cr, prediction_normalized_cr)\n",
        "        container_mse = container_mse.write(idx, mse)\n",
        "        mean, std, ssim = compute_ssim(ground_truth_normalized_cr, prediction_normalized_cr)\n",
        "        container_ssim = container_ssim.write(idx, ssim)\n",
        "\n",
        "        if evaluate_tumor_area:\n",
        "            ground_truth_masked_normalized, prediction_masked_normalized = retrieve_tumor_area(ground_truth_normalized_cr, prediction_normalized_cr, segmentation_cr)\n",
        "            mean, std, psnr = compute_psnr(ground_truth_masked_normalized, prediction_masked_normalized)\n",
        "            container_psnr_tumor = container_psnr_tumor.write(idx, psnr)\n",
        "            mean, std, mse = compute_mse(ground_truth_masked_normalized, prediction_masked_normalized)\n",
        "            container_mse_tumor = container_mse_tumor.write(idx, mse)\n",
        "            mean, std, ssim = compute_ssim(ground_truth_masked_normalized, prediction_masked_normalized)\n",
        "            container_ssim_tumor = container_ssim_tumor.write(idx, ssim)\n",
        "\n",
        "       # idx += 1\n",
        "    container_psnr = container_psnr.stack()\n",
        "    container_mse = container_mse.stack()\n",
        "    container_ssim = container_ssim.stack()\n",
        "    mean_psnr = tf.reduce_mean(tf.boolean_mask((container_psnr), tf.math.is_finite(container_psnr)))\n",
        "    std_psnr = tf.math.reduce_std(tf.boolean_mask((container_psnr), tf.math.is_finite(container_psnr)))\n",
        "    mean_mse = tf.reduce_mean(tf.boolean_mask((container_mse), tf.math.is_finite(container_mse)))\n",
        "    std_mse = tf.math.reduce_std(tf.boolean_mask((container_mse), tf.math.is_finite(container_mse)))\n",
        "    mean_ssim = tf.reduce_mean(tf.boolean_mask((container_ssim), tf.math.is_finite(container_ssim)))\n",
        "    std_ssim = tf.math.reduce_std(tf.boolean_mask((container_ssim), tf.math.is_finite(container_ssim)))\n",
        "\n",
        "    print(\"PSNR on {} set: {} ± {}\".format(set_type, (f'{mean_psnr:.4f}'), (f'{std_psnr:.4f}')))\n",
        "    print(\"MSE on {} set: {} ± {}\".format(set_type, (f'{mean_mse:.4f}'), (f'{std_mse:.4f}')))\n",
        "    print(\"SSIM on {} set: {} ± {}\".format(set_type, (f'{mean_ssim:.4f}'), (f'{std_ssim:.4f}')))\n",
        "\n",
        "    if evaluate_tumor_area:\n",
        "        container_psnr_tumor = container_psnr_tumor.stack()\n",
        "        container_mse_tumor = container_mse_tumor.stack()\n",
        "        container_ssim_tumor = container_ssim_tumor.stack()\n",
        "        mean_psnr_tumor = tf.reduce_mean(tf.boolean_mask((container_psnr_tumor), tf.math.is_finite(container_psnr_tumor)))\n",
        "        std_psnr_tumor = tf.math.reduce_std(tf.boolean_mask((container_psnr_tumor), tf.math.is_finite(container_psnr_tumor)))\n",
        "        mean_mse_tumor = tf.reduce_mean(tf.boolean_mask((container_mse_tumor), tf.math.is_finite(container_mse_tumor)))\n",
        "        std_mse_tumor = tf.math.reduce_std(tf.boolean_mask((container_mse_tumor), tf.math.is_finite(container_mse_tumor)))\n",
        "        mean_ssim_tumor = tf.reduce_mean(tf.boolean_mask((container_ssim_tumor), tf.math.is_finite(container_ssim_tumor)))\n",
        "        std_ssim_tumor = tf.math.reduce_std(tf.boolean_mask((container_ssim_tumor), tf.math.is_finite(container_ssim_tumor)))\n",
        "        print()\n",
        "        print(\"PSNR wrt tumor area on {} set: {} ± {}\".format(set_type, (f'{mean_psnr_tumor:.4f}'), (f'{std_psnr_tumor:.4f}')))\n",
        "        print(\"MSE wrt tumor area on {} set: {} ± {}\".format(set_type, (f'{mean_mse_tumor:.4f}'), (f'{std_mse_tumor:.4f}')))\n",
        "        print(\"SSIM wrt tumor area on {} set: {} ± {}\".format(set_type, (f'{mean_ssim_tumor:.4f}'), (f'{std_ssim_tumor:.4f}')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQ4xMZYCehpZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "8127191a-1717-48fd-d420-6b163ec07482"
      },
      "source": [
        "evaluate_GAN(dataset=testing, set_type='test', first_sequence=1, second_sequence=1, evaluate_tumor_area=True)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PSNR on test set: nan ± nan\n",
            "MSE on test set: 0.0000 ± 0.0000\n",
            "SSIM on test set: 1.0000 ± 0.0000\n",
            "\n",
            "PSNR wrt tumor area on test set: nan ± nan\n",
            "MSE wrt tumor area on test set: 0.0000 ± 0.0000\n",
            "SSIM wrt tumor area on test set: 1.0000 ± 0.0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SRYbeH8dE85",
        "colab_type": "text"
      },
      "source": [
        "#T1 and T2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBqecSREarpx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "f8e5bd7f-1142-4a30-9940-f8f16b935ee2"
      },
      "source": [
        "evaluate_GAN(dataset=testing, set_type='test', first_sequence=0, second_sequence=1, evaluate_tumor_area=True)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PSNR on test set: 15.3286 ± 4.2134\n",
            "MSE on test set: 0.0396 ± 0.0275\n",
            "SSIM on test set: 0.5054 ± 0.2116\n",
            "\n",
            "PSNR wrt tumor area on test set: 28.5796 ± 8.4685\n",
            "MSE wrt tumor area on test set: 0.0023 ± 0.0046\n",
            "SSIM wrt tumor area on test set: 0.9690 ± 0.0449\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GY7x7Q-QdOrL",
        "colab_type": "text"
      },
      "source": [
        "#T1 and T1c"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_tnjQehCdC2Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "3a198017-54a8-4c43-cf31-6de1d7797940"
      },
      "source": [
        "evaluate_GAN(dataset=testing, set_type='test', first_sequence=0, second_sequence=2, evaluate_tumor_area=True)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PSNR on test set: 23.8431 ± 4.0912\n",
            "MSE on test set: 0.0058 ± 0.0050\n",
            "SSIM on test set: 0.8096 ± 0.0984\n",
            "\n",
            "PSNR wrt tumor area on test set: 35.0662 ± 9.4743\n",
            "MSE wrt tumor area on test set: 0.0007 ± 0.0014\n",
            "SSIM wrt tumor area on test set: 0.9833 ± 0.0272\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbKS3b6JdRXH",
        "colab_type": "text"
      },
      "source": [
        "#T1 and T2Flair"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4g0EhODqdTMN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "2701d3fe-d27a-473d-9e16-38c4f88380f4"
      },
      "source": [
        "evaluate_GAN(dataset=testing, set_type='test', first_sequence=0, second_sequence=3, evaluate_tumor_area=True)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PSNR on test set: 19.9617 ± 4.9249\n",
            "MSE on test set: 0.0155 ± 0.0131\n",
            "SSIM on test set: 0.6876 ± 0.1628\n",
            "\n",
            "PSNR wrt tumor area on test set: 29.5316 ± 8.8640\n",
            "MSE wrt tumor area on test set: 0.0019 ± 0.0032\n",
            "SSIM wrt tumor area on test set: 0.9786 ± 0.0324\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FanfTEQWdmjw",
        "colab_type": "text"
      },
      "source": [
        "#T2 and T1c"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-o0C00jQdiFo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "2ba64d5d-33b0-429b-d5d3-119e180e8cf0"
      },
      "source": [
        "evaluate_GAN(dataset=testing, set_type='test', first_sequence=1, second_sequence=2, evaluate_tumor_area=True)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PSNR on test set: 15.5055 ± 4.2370\n",
            "MSE on test set: 0.0387 ± 0.0281\n",
            "SSIM on test set: 0.5209 ± 0.2061\n",
            "\n",
            "PSNR wrt tumor area on test set: 28.2339 ± 8.6849\n",
            "MSE wrt tumor area on test set: 0.0027 ± 0.0054\n",
            "SSIM wrt tumor area on test set: 0.9679 ± 0.0469\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2YXUIsXdoDB",
        "colab_type": "text"
      },
      "source": [
        "#T2 and T2Flair"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbHfFvtgd41R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "4e3c84ab-3dee-4979-facf-842a9f70a919"
      },
      "source": [
        "evaluate_GAN(dataset=testing, set_type='test', first_sequence=1, second_sequence=3, evaluate_tumor_area=True)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PSNR on test set: 16.8268 ± 3.9727\n",
            "MSE on test set: 0.0275 ± 0.0199\n",
            "SSIM on test set: 0.6262 ± 0.1597\n",
            "\n",
            "PSNR wrt tumor area on test set: 30.0709 ± 7.6617\n",
            "MSE wrt tumor area on test set: 0.0014 ± 0.0033\n",
            "SSIM wrt tumor area on test set: 0.9790 ± 0.0309\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GO5Ydkh5d8ww",
        "colab_type": "text"
      },
      "source": [
        "#T1c and T2Flair"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-5FJNZHd-yj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "c72ab93e-c730-4aee-9ae5-993714c72f39"
      },
      "source": [
        "evaluate_GAN(dataset=testing, set_type='test', first_sequence=2, second_sequence=3, evaluate_tumor_area=True)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "PSNR on test set: 19.0751 ± 4.2967\n",
            "MSE on test set: 0.0171 ± 0.0114\n",
            "SSIM on test set: 0.6562 ± 0.1678\n",
            "\n",
            "PSNR wrt tumor area on test set: 28.7030 ± 8.9467\n",
            "MSE wrt tumor area on test set: 0.0023 ± 0.0040\n",
            "SSIM wrt tumor area on test set: 0.9748 ± 0.0384\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
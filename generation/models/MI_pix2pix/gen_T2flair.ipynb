{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "gen_T2flair.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPDCzCA5c50G",
        "colab_type": "text"
      },
      "source": [
        "## Import libraries and mount the drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngnfgyxdgvDv",
        "colab_type": "code",
        "outputId": "5ea7ff20-6a25-4042-b86c-8fa91a7a3f87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from IPython import display\n",
        "import time\n",
        "import math\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc3\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2TYHyiV-t_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('./drive/My Drive/Master_thesis/generation')\n",
        "from dataset_helpers import load_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfK94uPI--2S",
        "colab_type": "text"
      },
      "source": [
        "## Loading train, validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvRByzzuUfst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_datasets():\n",
        "    validation = load_dataset(prefix_path + 'brats2015_validation_crop_mri.tfrecords', ['MR_T1', 'MR_T1c', 'MR_T2', 'MR_Flair'], batch_size=BATCH_SIZE, shuffle=False)\n",
        "    training = load_dataset(prefix_path + 'brats2015_training_crop_mri.tfrecords', ['MR_T1', 'MR_T1c', 'MR_T2', 'MR_Flair'], batch_size=BATCH_SIZE, shuffle=True)\n",
        "    testing = load_dataset(prefix_path + 'brats2015_testing_crop_mri.tfrecords', ['MR_T1', 'MR_T1c', 'MR_T2', 'MR_Flair'], batch_size=BATCH_SIZE, shuffle=True)\n",
        "    return training, validation , testing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5la9-anCd8-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prefix_path = './drive/My Drive/Master_thesis/datasets/'\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "training, validation, testing = load_datasets()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEOWtEya_HY6",
        "colab_type": "text"
      },
      "source": [
        "## Load some useful functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILemx0Oz3ZBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resize_with_crop(*args): # possibile arguments: input, gt, prediction and maybe the segmentation\n",
        "\n",
        "    image0 = tf.image.resize_with_crop_or_pad(args[0], 155, 194)\n",
        "    image1 = tf.image.resize_with_crop_or_pad(args[1], 155, 194)\n",
        "    image2 = tf.image.resize_with_crop_or_pad(args[2], 155, 194)\n",
        "    image3 = tf.image.resize_with_crop_or_pad(args[3], 155, 194)\n",
        "    if len(args) == 5:      # crop also the segmentation, if is given as additional argument\n",
        "        image4 = tf.image.resize_with_crop_or_pad(args[4], 155, 194)\n",
        "        return image0, image1, image2, image3, image4\n",
        "    if len(args) == 6:\n",
        "        image4 = tf.image.resize_with_crop_or_pad(args[4], 155, 194)\n",
        "        image5 = tf.image.resize_with_crop_or_pad(args[5], 155, 194)\n",
        "        return image0, image1, image2, image3, image4, image5\n",
        "    return image0, image1, image2, image3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kX5bu_es7zec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def retrieve_tumor_area(ground_truth, prediction, segmentation):\n",
        "    ground_truth_np = ground_truth.numpy()\n",
        "    segmentation_np = segmentation.numpy()\n",
        "    prediction_np = prediction.numpy()\n",
        "\n",
        "    # I want to remove all the pixels not relevant wrt the tumor area. \n",
        "    idx = (segmentation_np==0)      \n",
        "    ground_truth_np[idx] = segmentation_np[idx]\n",
        "    prediction_np[idx] = segmentation_np[idx]\n",
        "\n",
        "    return ground_truth_np, prediction_np      # Now the images are ready to be evaluated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eS4RxJF_zVW",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svxcGvwIW_Di",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_GAN(model, dataset, set_type, evaluate_tumor_area=False, print_loss=False):\n",
        "    num_batches = 0                 # CAREFUL when batch_size is modified!!!\n",
        "    if set_type == 'test':\n",
        "        num_batches = 112           # in the test set, with batch 32, there are 112 elements.\n",
        "    elif set_type == 'validation':\n",
        "        num_batches = 108\n",
        "    elif set_type == 'train':\n",
        "        num_batches = 876\n",
        "    \n",
        "    container_psnr = tf.TensorArray(tf.float32, size=num_batches)     \n",
        "    container_mse = tf.TensorArray(tf.float32, size=num_batches)\n",
        "    container_ssim = tf.TensorArray(tf.float32, size=num_batches)\n",
        "    \n",
        "    if evaluate_tumor_area:\n",
        "        container_psnr_tumor = tf.TensorArray(tf.float32, size=num_batches)     \n",
        "        container_mse_tumor = tf.TensorArray(tf.float32, size=num_batches)\n",
        "        container_ssim_tumor = tf.TensorArray(tf.float32, size=num_batches)\n",
        "    if print_loss:\n",
        "        container_gen = tf.TensorArray(tf.float32, size=num_batches)     \n",
        "        container_disc = tf.TensorArray(tf.float32, size=num_batches)\n",
        "    \n",
        "    for idx, (t1, t2, t1c, t2flair, segmentation, patient) in dataset.enumerate():\n",
        "        # preparing the input to the generator\n",
        "        ground_truth = t2flair\n",
        "        input_tensor = tf.concat([t1, t2, t1c], 3)\n",
        "\n",
        "        # generate the prediction            \n",
        "        prediction = model(input_tensor, training=False)\n",
        "        \n",
        "        t1_cr, t2_cr, t1c_cr, gt_cr, pred_cr, segmentation_cr = resize_with_crop(t1, t2, t1c, ground_truth, prediction, segmentation)\n",
        "        \n",
        "        if evaluate_tumor_area:\n",
        "            prediction_normalized = mean_normalize(pred_cr)\n",
        "            ground_truth_normalized = mean_normalize(gt_cr) \n",
        "            ground_truth_masked, prediction_masked = retrieve_tumor_area(ground_truth_normalized, prediction_normalized, segmentation_cr)\n",
        "        \n",
        "        # PLOT ALWAYS THE SAME IMAGES, SO THAT IT'S EASIER TO PERCEIVE DIFFERENCES IN THE PREDICTIONS BETWEEN EPOCHS.\n",
        "        if (idx == 2 or idx == 25 or idx == 45 or idx == 50 or idx == 83) and evaluate_tumor_area:\n",
        "                plot_images(t1_cr, t2_cr, t1c_cr, gt_cr, pred_cr, ground_truth_masked, prediction_masked)\n",
        "        elif idx == 2 or idx == 25 or idx == 45 or idx == 50 or idx == 83:\n",
        "                plot_images(t1_cr, t2_cr, t1c_cr, gt_cr, pred_cr)\n",
        "\n",
        "        # normalize the prediction and the ground_truth\n",
        "        prediction_normalized_cr = mean_normalize(pred_cr)\n",
        "        ground_truth_normalized_cr = mean_normalize(gt_cr)\n",
        "\n",
        "        # compute the metrics of similarity\n",
        "        mean, std, psnr = compute_psnr(ground_truth_normalized_cr, prediction_normalized_cr)\n",
        "        container_psnr = container_psnr.write(idx, psnr)\n",
        "        mean, std, mse = compute_mse(ground_truth_normalized_cr, prediction_normalized_cr)\n",
        "        container_mse = container_mse.write(idx, mse)\n",
        "        mean, std, ssim = compute_ssim(ground_truth_normalized_cr, prediction_normalized_cr)\n",
        "        container_ssim = container_ssim.write(idx, ssim)\n",
        "\n",
        "        if evaluate_tumor_area:\n",
        "            mean, std, psnr = compute_psnr_tumor(ground_truth_masked, prediction_masked)\n",
        "            container_psnr_tumor = container_psnr_tumor.write(idx, psnr)\n",
        "            mean, std, mse = compute_mse_tumor(ground_truth_masked, prediction_masked)\n",
        "            container_mse_tumor = container_mse_tumor.write(idx, mse)\n",
        "            mean, std, ssim = compute_ssim(ground_truth_masked, prediction_masked)\n",
        "            container_ssim_tumor = container_ssim_tumor.write(idx, ssim)\n",
        "\n",
        "        # COMPUTES VALIDATION LOSSES FOR THE GENERATOR AND THE DISCRIMINATOR\n",
        "        if print_loss:\n",
        "            ####################################\n",
        "            disc_real_output = discriminator([input_tensor, ground_truth], training=True)\n",
        "                \n",
        "            # showing to D a batch fake images of T2\n",
        "            disc_generated_output = discriminator([input_tensor, prediction], training=True)\n",
        "\n",
        "            gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, prediction, ground_truth)\n",
        "            \n",
        "            disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "            container_gen = container_gen.write(idx, gen_total_loss)\n",
        "            container_disc = container_disc.write(idx, disc_loss)\n",
        "            ###############################\n",
        "\n",
        "    container_psnr = container_psnr.stack()\n",
        "    container_mse = container_mse.stack()\n",
        "    container_ssim = container_ssim.stack()\n",
        "    mean_psnr = tf.reduce_mean(tf.boolean_mask((container_psnr), tf.math.is_finite(container_psnr)))\n",
        "    std_psnr = tf.math.reduce_std(tf.boolean_mask((container_psnr), tf.math.is_finite(container_psnr)))\n",
        "    mean_mse = tf.reduce_mean(tf.boolean_mask((container_mse), tf.math.is_finite(container_mse)))\n",
        "    std_mse = tf.math.reduce_std(tf.boolean_mask((container_mse), tf.math.is_finite(container_mse)))\n",
        "    mean_ssim = tf.reduce_mean(tf.boolean_mask((container_ssim), tf.math.is_finite(container_ssim)))\n",
        "    std_ssim = tf.math.reduce_std(tf.boolean_mask((container_ssim), tf.math.is_finite(container_ssim)))\n",
        "\n",
        "    print(\"PSNR on {} set: {} ± {}\".format(set_type, (f'{mean_psnr:.4f}'), (f'{std_psnr:.4f}')))\n",
        "    print(\"MSE on {} set: {} ± {}\".format(set_type, (f'{mean_mse:.4f}'), (f'{std_mse:.4f}')))\n",
        "    print(\"SSIM on {} set: {} ± {}\".format(set_type, (f'{mean_ssim:.4f}'), (f'{std_ssim:.4f}')))\n",
        "\n",
        "    if evaluate_tumor_area:\n",
        "        container_psnr_tumor = container_psnr_tumor.stack()\n",
        "        container_mse_tumor = container_mse_tumor.stack()\n",
        "        container_ssim_tumor = container_ssim_tumor.stack()\n",
        "        \n",
        "        threshold = 228 # I don't consider values with PSNR > 228, because it means we are looking basically at black images.\n",
        "        container_psnr_tumor = tf.boolean_mask((container_psnr_tumor), tf.math.is_finite(container_psnr_tumor))\n",
        "        container_psnr_tumor = container_psnr_tumor[container_psnr_tumor < threshold]\n",
        "\n",
        "        mean_psnr_tumor = tf.reduce_mean(container_psnr_tumor)\n",
        "        std_psnr_tumor = tf.math.reduce_std(container_psnr_tumor)\n",
        "        mean_mse_tumor = tf.reduce_mean(tf.boolean_mask((container_mse_tumor), tf.math.is_finite(container_mse_tumor)))\n",
        "        std_mse_tumor = tf.math.reduce_std(tf.boolean_mask((container_mse_tumor), tf.math.is_finite(container_mse_tumor)))\n",
        "        mean_ssim_tumor = tf.reduce_mean(tf.boolean_mask((container_ssim_tumor), tf.math.is_finite(container_ssim_tumor)))\n",
        "        std_ssim_tumor = tf.math.reduce_std(tf.boolean_mask((container_ssim_tumor), tf.math.is_finite(container_ssim_tumor)))\n",
        "        print()\n",
        "        print(\"PSNR wrt tumor area on {} set: {} ± {}\".format(set_type, (f'{mean_psnr_tumor:.4f}'), (f'{std_psnr_tumor:.4f}')))\n",
        "        print(\"MSE wrt tumor area on {} set: {} ± {}\".format(set_type, (f'{mean_mse_tumor:.4f}'), (f'{std_mse_tumor:.4f}')))\n",
        "        print(\"SSIM wrt tumor area on {} set: {} ± {}\".format(set_type, (f'{mean_ssim_tumor:.4f}'), (f'{std_ssim_tumor:.4f}')))\n",
        "    if print_loss:\n",
        "        container_disc = container_disc.stack()\n",
        "        container_gen = container_gen.stack()\n",
        "        mean_gen = tf.reduce_mean(tf.boolean_mask((container_gen), tf.math.is_finite(container_gen)))\n",
        "        std_gen = tf.math.reduce_std(tf.boolean_mask((container_gen), tf.math.is_finite(container_gen)))\n",
        "        mean_disc = tf.reduce_mean(tf.boolean_mask((container_disc), tf.math.is_finite(container_disc)))\n",
        "        std_disc = tf.math.reduce_std(tf.boolean_mask((container_disc), tf.math.is_finite(container_disc)))\n",
        "        print()\n",
        "        print(\"Generator loss on {} set: {} ± {}\".format(set_type, (f'{mean_gen:.4f}'), (f'{std_gen:.4f}')))\n",
        "        print(\"Discriminator loss on {} set: {} ± {}\".format(set_type, (f'{mean_disc:.4f}'), (f'{std_disc:.4f}')))\n",
        "        \n",
        "    if tf.equal(print_loss, True) and tf.equal(evaluate_tumor_area, False):\n",
        "        return mean_psnr, mean_mse, mean_ssim, mean_gen, mean_disc\n",
        "    elif tf.equal(print_loss, False) and tf.equal(evaluate_tumor_area, True):\n",
        "        return mean_psnr, mean_mse, mean_ssim, mean_psnr_tumor, mean_mse_tumor, mean_ssim_tumor\n",
        "    elif tf.equal(print_loss, True) and tf.equal(evaluate_tumor_area, True):\n",
        "        return mean_psnr, mean_mse, mean_ssim, mean_psnr_tumor, mean_mse_tumor, mean_ssim_tumor, mean_gen, mean_disc\n",
        "    return mean_psnr, mean_mse, mean_ssim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WAnSHCwo_CT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_loss(g_l, d_l):        \n",
        "    f, (ax1) = plt.subplots(1, 1, figsize=(12, 6))\n",
        "    f.subplots_adjust(hspace=0.4)\n",
        "    max_epoch = g_l.shape[0]       # args[0] \n",
        "    epoch_list = list(range(1,max_epoch+1))\n",
        "    ax1.plot(epoch_list, g_l, label='Generator loss')\n",
        "    ax1.plot(epoch_list, d_l, label='Discriminator Loss')\n",
        "    ax1.set_xticks(np.arange(1, max_epoch, 5))\n",
        "    ax1.set_ylabel('Loss Value')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_title('LOSS on validation set')\n",
        "    ax1.set_ylim([0.0, 12.0])\n",
        "    l1 = ax1.legend(loc=\"best\")\n",
        "    \n",
        "def plot_metrics(*args):        # arguments will be, in order: psnr, mse, ssim, psnr_tumor*, mse_tumor*, ssim_tumor*\n",
        "    tumor_area = False          # arguments with * are optional\n",
        "    if len(args) == 6:\n",
        "        tumor_area = True\n",
        "\n",
        "    f, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 20))\n",
        "    f.subplots_adjust(hspace=0.4)\n",
        "\n",
        "    max_epoch = (args[0]).shape[0]       # args[0] \n",
        "\n",
        "    epoch_list = list(range(1,max_epoch+1))\n",
        "    ax1.plot(epoch_list, args[0], label='PSNR')\n",
        "    if tumor_area: \n",
        "        ax1.plot(epoch_list, args[3], label='PSNR on tumor area')\n",
        "    ax1.set_xticks(np.arange(1, max_epoch, 5))\n",
        "    ax1.set_ylabel('PSNR Value')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_title('PSNR')\n",
        "    ax1.set_ylim([9, 35])\n",
        "    l1 = ax1.legend(loc=\"best\")\n",
        "\n",
        "    ax2.plot(epoch_list, args[1], label='MSE')\n",
        "    if tumor_area: \n",
        "        ax2.plot(epoch_list, args[4], label='MSE on tumor area')\n",
        "    ax2.set_xticks(np.arange(1, max_epoch, 5))\n",
        "    ax2.set_ylabel('MSE Value')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_title('MSE')\n",
        "    ax2.set_ylim([0.0, 0.10])\n",
        "    l2 = ax2.legend(loc=\"best\")\n",
        "\n",
        "    ax3.plot(epoch_list, args[2], label='SSIM')\n",
        "    if tumor_area: \n",
        "        ax3.plot(epoch_list, args[5], label='SSIM on tumor area')\n",
        "    ax3.set_xticks(np.arange(1, max_epoch, 5))\n",
        "    ax3.set_ylabel('SSIM Value')\n",
        "    ax3.set_xlabel('Epoch')\n",
        "    ax3.set_title('SSIM')\n",
        "    ax3.set_ylim([0.0, 1.0])\n",
        "    l3 = ax3.legend(loc=\"best\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioO19OjSm0Y3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_images(*args): \n",
        "\n",
        "    if len(args) == 5:\n",
        "        display_list = [args[0], args[1], args[2], args[3], args[4]]   # input, gt, prediction, gt masked, pred masked\n",
        "        title = ['T1', 'T2', 'T1c', 'T2flair - ground Truth', 'T2flair - Predicted Image',]\n",
        "        figsize = (14, 7)\n",
        "    elif len(args) == 3:\n",
        "        display_list = [args[0], args[1], args[2]]   # input, gt and prediction\n",
        "        title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
        "        figsize = (8, 4)\n",
        "    elif len(args) == 7:\n",
        "        display_list = [args[0], args[1], args[2], args[3], args[4], args[5], args[6]]   # input, gt and prediction\n",
        "        title = ['T1', 'T2', 'T1c', 'T2flair - ground Truth', 'T2flair - Predicted Image', 'GT Tumor', 'Pred Tumor']\n",
        "        figsize = (22, 6)\n",
        "    \n",
        "    plt.figure(1 , figsize)\n",
        "    n = 0\n",
        "    for i in range(len(args)):    # batch size is different from 10, but let's show just 10 images.\n",
        "        n += 1\n",
        "        plt.subplot(1, len(args), n).title.set_text(title[i])\n",
        "        plt.imshow(tf.squeeze(display_list[i][0]), cmap='bone')\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98iV1zqoex77",
        "colab_type": "text"
      },
      "source": [
        "## Mean normalization\n",
        "\n",
        "![alt text](https://wikimedia.org/api/rest_v1/media/math/render/svg/5c591a0eeba163a12f69f937adbae5886d6273db)\n",
        "\n",
        "In the paper they say: \"Each patient scan is normalized by dividing each sequence by its mean intensity value. \"\n",
        "But the formula is taken from a lecture from Andrew Ng, where he defines the Mean normalization as in the formula above. (resource: https://www.youtube.com/watch?v=e1nTgoDI_m8)\n",
        "\n",
        "See also: https://stats.stackexchange.com/questions/138046/normalizations-dividing-by-mean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BURIoajNRLPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mean_rescale(x, xmin, xmax):\n",
        "    mean = tf.reduce_mean(x)\n",
        "    return ((x-mean)/(xmax-xmin)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dT_FH4yRI4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def mean_normalize(image):\n",
        "    image_normalized = tf.TensorArray(tf.float32, size=BATCH_SIZE)\n",
        "    for i in range(BATCH_SIZE):\n",
        "        # rescaling each image in the batch\n",
        "        max_value = tf.math.reduce_max(image[i])\n",
        "        min_value = tf.math.reduce_min(image[i])\n",
        "        x = mean_rescale(image[i], min_value, max_value)\n",
        "        image_normalized = image_normalized.write(i, x)\n",
        "    image_normalized = image_normalized.stack()\n",
        "    return image_normalized\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NDDqVvqABDm",
        "colab_type": "text"
      },
      "source": [
        "## Discard black images from batch (put values to 'nan')\n",
        "\n",
        "This normalization is just to test the metrics and see if there is a big difference in normalizing the prediction and the gt.\n",
        "This method is used to normalize (and so put to 'nan') only the black images, while the other images of the batch are kept with the original values. This allows me to discard the black images in the computation of the metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVFWdUV2-3-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def black_images_to_nan(image): \n",
        "    image_normalized = tf.TensorArray(tf.float32, size=BATCH_SIZE)\n",
        "    for i in range(BATCH_SIZE):\n",
        "        # rescaling each image in the batch\n",
        "        max_value = tf.math.reduce_max(image[i])\n",
        "        min_value = tf.math.reduce_min(image[i])\n",
        "\n",
        "        # if the max = min most likely it's a black image (or an image without any important information)\n",
        "        if tf.math.equal(max_value, min_value):        \n",
        "            x = mean_rescale(image[i], min_value, max_value)\n",
        "            image_normalized = image_normalized.write(i, x)\n",
        "        else:\n",
        "            image_normalized = image_normalized.write(i, image[i])\n",
        "    image_normalized = image_normalized.stack()\n",
        "    return image_normalized"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QIl04K0PMurE",
        "colab_type": "text"
      },
      "source": [
        "##Defining all the quantitative metrics (PSNR, SSIM and MSE)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BUtrXxJe3Xvc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_psnr(image1, image2):\n",
        "    # Compute PSNR over tf.float32 Tensors.\n",
        "    im1 = tf.image.convert_image_dtype(image1, tf.float32)   # inp is a numpy.ndarray and im1 is an EagerTensor\n",
        "    im2 = tf.image.convert_image_dtype(image2, tf.float32)\n",
        "    psnr = tf.image.psnr(im1, im2, max_val=1.0)\n",
        "    mean = tf.reduce_mean(tf.boolean_mask((psnr), tf.math.is_finite(psnr)))\n",
        "    std = tf.math.reduce_std(tf.boolean_mask((psnr), tf.math.is_finite(psnr)))\n",
        "    # In the computation of mean and std, I'm ignoring the 'nan' and 'inf' values\n",
        "    # Why 'nan' values? 'nan' happens when there is an image with max_value and min_value = 0.0 so a black image\n",
        "    # the PSNR would be inf (image is totally similar to the ground truth)\n",
        "    # be rescaling the image, the max_value and min_value would become nan and so the PSNR\n",
        "\n",
        "    # It ignores also the 'inf' values, in the case I don't want to normalize\n",
        "    \n",
        "    return mean, std, psnr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0kHZvJ6LhG3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_ssim(image1, image2):\n",
        "    im1 = tf.image.convert_image_dtype(image1, tf.float32)   # inp is a numpy.ndarray and im1 is an EagerTensor\n",
        "    im2 = tf.image.convert_image_dtype(image2, tf.float32)\n",
        "    ssim = tf.image.ssim(im1, im2, max_val=1)\n",
        "    mean = tf.reduce_mean(tf.boolean_mask((ssim), tf.math.is_finite(ssim)))\n",
        "    std = tf.math.reduce_std(tf.boolean_mask((ssim), tf.math.is_finite(ssim)))\n",
        "    #print(ssim)\n",
        "    return mean, std, ssim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_kkj3HwPtK5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# I want to compute, first thing, the MSE between ground truth and generated one. The tf.function gives me a Tensor 32x256x256:\n",
        "# MSE is computed PIXEL per PIXEL, so per each of the 32 matrix 256x256, I average (1) the values of the 256x256 pixels obtaining\n",
        "# an array of 32 elements, containing the MSEs of each image belonging to the batch. Then I can average (2) these 32 to have \n",
        "# I should not average the whole 32x256x256 in one step. The result would have same mean but slightly different std.\n",
        "# I want first to obtain the MSE of each image... then I average across the batch only to have smth more accurate.\n",
        "\n",
        "def compute_mse(image1, image2):       # mean squared error\n",
        "    im1 = tf.image.convert_image_dtype(image1, tf.float32)   # inp is a numpy.ndarray and im1 is an EagerTensor\n",
        "    im2 = tf.image.convert_image_dtype(image2, tf.float32)\n",
        "    mse = tf.metrics.mean_squared_error(im1,im2)\n",
        "    # In this way is possible to do Variable item-assignment with tensors \n",
        "    mse_per_image = tf.TensorArray(tf.float32, size=BATCH_SIZE)\n",
        "    for i in range(BATCH_SIZE):\n",
        "        x = tf.reduce_mean(tf.boolean_mask((mse[i]), tf.math.is_finite(mse[i]))) \n",
        "        mse_per_image = mse_per_image.write(i, x)\n",
        "    mse_per_image = mse_per_image.stack()\n",
        "    mean = tf.reduce_mean(tf.boolean_mask((mse_per_image), tf.math.is_finite(mse_per_image)))\n",
        "    std = tf.math.reduce_std(tf.boolean_mask((mse_per_image), tf.math.is_finite(mse_per_image)))\n",
        "    return mean, std, mse_per_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTnT4_Qpb1aj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.python.ops import math_ops\n",
        "\n",
        "def compute_mse_tumor(image1, image2):       # mean squared error\n",
        "    im1 = tf.image.convert_image_dtype(image1, tf.float32)   # inp is a numpy.ndarray and im1 is an EagerTensor\n",
        "    im2 = tf.image.convert_image_dtype(image2, tf.float32)\n",
        "    squared_difference = tf.math.squared_difference(im1,im2)\n",
        "    mse_per_image = tf.TensorArray(tf.float32, size=BATCH_SIZE)\n",
        "    for i in range(BATCH_SIZE):\n",
        "        non_zero_elements = tf.math.count_nonzero(squared_difference[i], dtype=tf.dtypes.float32)\n",
        "        sum_over_squared_difference = tf.math.reduce_sum(squared_difference[i])\n",
        "        x = tf.math.divide(sum_over_squared_difference, non_zero_elements) \n",
        "\n",
        "        mse_per_image = mse_per_image.write(i, x)\n",
        "    mse_per_image = mse_per_image.stack()\n",
        "    mean = tf.reduce_mean(tf.boolean_mask((mse_per_image), tf.math.is_finite(mse_per_image)))\n",
        "    std = tf.math.reduce_std(tf.boolean_mask((mse_per_image), tf.math.is_finite(mse_per_image)))\n",
        "    return mean, std, mse_per_image\n",
        "\n",
        "\n",
        "def compute_psnr_tumor(image1, image2):\n",
        "    max_val=1.0\n",
        "    im1 = tf.image.convert_image_dtype(image1, tf.float32)\n",
        "    im2 = tf.image.convert_image_dtype(image2, tf.float32)\n",
        "    squared_difference = tf.math.squared_difference(im1,im2)\n",
        "    psnr_per_image = tf.TensorArray(tf.float32, size=BATCH_SIZE)\n",
        "    for i in range(BATCH_SIZE):\n",
        "        non_zero_elements = tf.math.count_nonzero(squared_difference[i], dtype=tf.dtypes.float32)\n",
        "        sum_over_squared_difference = tf.math.reduce_sum(squared_difference[i])\n",
        "        x = tf.math.divide(sum_over_squared_difference, non_zero_elements) \n",
        "        \n",
        "        psnr_val = math_ops.subtract(20 * math_ops.log(max_val) / math_ops.log(10.0), np.float32(10 / np.log(10)) * math_ops.log(x), name='psnr')\n",
        "        psnr_per_image = psnr_per_image.write(i, psnr_val)\n",
        "\n",
        "    psnr_per_image = psnr_per_image.stack()\n",
        "    mean = tf.reduce_mean(tf.boolean_mask((psnr_per_image), tf.math.is_finite(psnr_per_image)))\n",
        "    std = tf.math.reduce_std(tf.boolean_mask((psnr_per_image), tf.math.is_finite(psnr_per_image)))\n",
        "    return mean, std, psnr_per_image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-eVgrZJBpZ2",
        "colab_type": "text"
      },
      "source": [
        "## Defining models and optimizers - *G & D*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O8q36ZDLDyy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "OUTPUT_CHANNELS = 4\n",
        "LAMBDA = 100\n",
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Z495D1VkowP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def downsample(filters, size, apply_batchnorm=True):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(\n",
        "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                             kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "  if apply_batchnorm:\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  result.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeT7NojAZWm8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def upsample(filters, size, apply_dropout=False):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(\n",
        "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
        "                                    padding='same',\n",
        "                                    kernel_initializer=initializer,\n",
        "                                    use_bias=False))\n",
        "\n",
        "  result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  if apply_dropout:\n",
        "      result.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "  result.add(tf.keras.layers.ReLU())\n",
        "\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uj3P879duquT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hint from the web: Unfortunately, UNet does not work with arbitrary input sizes. Try ResNet instead.\n",
        "\n",
        "def Generator():\n",
        "  inputs = tf.keras.layers.Input(shape=[256,256,3], batch_size=BATCH_SIZE)\n",
        "\n",
        "  down_stack = [\n",
        "    downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64)\n",
        "    downsample(128, 4), # (bs, 64, 64, 128)\n",
        "    downsample(256, 4), # (bs, 32, 32, 256)\n",
        "    downsample(512, 4), # (bs, 16, 16, 512)\n",
        "    downsample(512, 4), # (bs, 8, 8, 512)\n",
        "    downsample(512, 4), # (bs, 4, 4, 512)\n",
        "    downsample(512, 4), # (bs, 2, 2, 512)\n",
        "    downsample(512, 4), # (bs, 1, 1, 512)\n",
        "  ]\n",
        "  up_stack = [\n",
        "    upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n",
        "    upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n",
        "    upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n",
        "    upsample(512, 4), # (bs, 16, 16, 1024)\n",
        "    upsample(256, 4), # (bs, 32, 32, 512)\n",
        "    upsample(128, 4), # (bs, 64, 64, 256)\n",
        "    upsample(64, 4), # (bs, 128, 128, 128)\n",
        "  ]\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "  last = tf.keras.layers.Conv2DTranspose(1, 4, strides=2, padding='same',\n",
        "                                         kernel_initializer=initializer,activation='tanh') # (bs, 256, 256, 1)\n",
        "  x = inputs\n",
        "\n",
        "  # Downsampling through the model\n",
        "  skips = []\n",
        "  for down in down_stack:\n",
        "    x = down(x)\n",
        "    skips.append(x)\n",
        "\n",
        "  skips = reversed(skips[:-1])\n",
        "\n",
        "  # Upsampling and establishing the skip connections\n",
        "  for up, skip in zip(up_stack, skips):\n",
        "    x = up(x)\n",
        "    x = tf.keras.layers.Concatenate()([x, skip])\n",
        "\n",
        "  x = last(x)\n",
        "\n",
        "  return tf.keras.Model(inputs=inputs, outputs=x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4McnkYFXGlpG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator_loss(disc_generated_output, gen_output, target):\n",
        "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "    # mean absolute error\n",
        "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
        "    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
        "\n",
        "\n",
        "    return total_gen_loss, gan_loss, l1_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHtWMOwiLYnl",
        "colab_type": "code",
        "outputId": "47e59c4b-2b1a-490c-eef6-98367fa05e24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "generator = Generator()\n",
        "generator.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_3 (InputLayer)            [(32, 256, 256, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential_36 (Sequential)      (32, 128, 128, 64)   3072        input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "sequential_37 (Sequential)      (32, 64, 64, 128)    131584      sequential_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sequential_38 (Sequential)      (32, 32, 32, 256)    525312      sequential_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sequential_39 (Sequential)      (32, 16, 16, 512)    2099200     sequential_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sequential_40 (Sequential)      (32, 8, 8, 512)      4196352     sequential_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sequential_41 (Sequential)      (32, 4, 4, 512)      4196352     sequential_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sequential_42 (Sequential)      (32, 2, 2, 512)      4196352     sequential_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sequential_43 (Sequential)      (32, 1, 1, 512)      4196352     sequential_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sequential_44 (Sequential)      (32, 2, 2, 512)      4196352     sequential_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_16 (Concatenate)    (32, 2, 2, 1024)     0           sequential_44[0][0]              \n",
            "                                                                 sequential_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sequential_45 (Sequential)      (32, 4, 4, 512)      8390656     concatenate_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_17 (Concatenate)    (32, 4, 4, 1024)     0           sequential_45[0][0]              \n",
            "                                                                 sequential_41[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sequential_46 (Sequential)      (32, 8, 8, 512)      8390656     concatenate_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_18 (Concatenate)    (32, 8, 8, 1024)     0           sequential_46[0][0]              \n",
            "                                                                 sequential_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sequential_47 (Sequential)      (32, 16, 16, 512)    8390656     concatenate_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_19 (Concatenate)    (32, 16, 16, 1024)   0           sequential_47[0][0]              \n",
            "                                                                 sequential_39[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sequential_48 (Sequential)      (32, 32, 32, 256)    4195328     concatenate_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_20 (Concatenate)    (32, 32, 32, 512)    0           sequential_48[0][0]              \n",
            "                                                                 sequential_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sequential_49 (Sequential)      (32, 64, 64, 128)    1049088     concatenate_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_21 (Concatenate)    (32, 64, 64, 256)    0           sequential_49[0][0]              \n",
            "                                                                 sequential_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sequential_50 (Sequential)      (32, 128, 128, 64)   262400      concatenate_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_22 (Concatenate)    (32, 128, 128, 128)  0           sequential_50[0][0]              \n",
            "                                                                 sequential_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_23 (Conv2DTran (32, 256, 256, 1)    2049        concatenate_22[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 54,421,761\n",
            "Trainable params: 54,410,881\n",
            "Non-trainable params: 10,880\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f91GydsSW6xG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Discriminator():\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image', batch_size=BATCH_SIZE)\n",
        "  tar = tf.keras.layers.Input(shape=[256, 256, 1], name='target_image', batch_size=BATCH_SIZE)\n",
        "\n",
        "  x = tf.keras.layers.concatenate([inp, tar]) # (bs, 256, 256, channels*2)\n",
        "\n",
        "  down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n",
        "  down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n",
        "  down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n",
        "\n",
        "  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n",
        "  conv = tf.keras.layers.Conv2D(512, 4, strides=1, kernel_initializer=initializer, use_bias=False)(zero_pad1) #(bs, 31, 31, 512)\n",
        "\n",
        "  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
        "\n",
        "  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
        "\n",
        "  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n",
        "\n",
        "  last = tf.keras.layers.Conv2D(1, 4, strides=1, kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n",
        "  return tf.keras.Model(inputs=[inp, tar], outputs=last)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD4gNdGPMb7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "\n",
        "  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "  total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "  return total_disc_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT0V9-gpXBfn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "discriminator = Discriminator()\n",
        "discriminator.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNBW-N3dsMWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B2IbpYM9G-v",
        "colab_type": "text"
      },
      "source": [
        "## Let's load now the generator, discriminator weights and optimizers status from the checkpoints\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIgHGlcZHdg7",
        "colab_type": "text"
      },
      "source": [
        "Uncomment this only to continue to train an already trained network. Otherwise it will be trained a new GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AN1i5bXlF8ss",
        "colab_type": "code",
        "outputId": "e079170b-f64f-44eb-a1e7-41c21962a05a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "print(\"Optimizers and models defined but not initialized? {}\".format(generator_optimizer.weights == []))\n",
        "\n",
        "checkpoint_path = \"./drive/My Drive/checkpoints/gen_tflair_v2\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                discriminator_optimizer=discriminator_optimizer,\n",
        "                                generator=generator,\n",
        "                                discriminator=discriminator)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print ('Latest checkpoint restored!!')\n",
        "\"\"\"\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nprint(\"Optimizers and models defined but not initialized? {}\".format(generator_optimizer.weights == []))\\n\\ncheckpoint_path = \"./drive/My Drive/checkpoints/gen_tflair_v2\"\\n\\nckpt = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\\n                                discriminator_optimizer=discriminator_optimizer,\\n                                generator=generator,\\n                                discriminator=discriminator)\\n\\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\\n\\n\\n# if a checkpoint exists, restore the latest checkpoint.\\n\\nif ckpt_manager.latest_checkpoint:\\n    ckpt.restore(ckpt_manager.latest_checkpoint)\\n    print (\\'Latest checkpoint restored!!\\')\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYaMDs5p-DwE",
        "colab_type": "text"
      },
      "source": [
        "Now the GAN is restored to the previous checkpoint and ready to be trained again !!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCGLZV21lSR2",
        "colab_type": "text"
      },
      "source": [
        "##**TRAIN STEP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrFHtXhmTMmp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(t1, t2, t1c, t2flair):\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "\n",
        "        ground_truth = t2flair\n",
        "        # INPUT IMPUTATION\n",
        "        input_tensor = tf.concat([t1, t2, t1c], 3)\n",
        "\n",
        "        # generate the prediction            \n",
        "        prediction = generator(input_tensor, training=False)\n",
        "        disc_real_output = discriminator([input_tensor, ground_truth], training=True)\n",
        "            \n",
        "        # showing to D a batch fake images of T2\n",
        "        disc_generated_output = discriminator([input_tensor, prediction], training=True)\n",
        "\n",
        "        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, prediction, ground_truth)\n",
        "        \n",
        "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "    generator_gradients = gen_tape.gradient(gen_total_loss,\n",
        "                                            generator.trainable_variables)\n",
        "    discriminator_gradients = disc_tape.gradient(disc_loss,\n",
        "                                                discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(generator_gradients,\n",
        "                                            generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
        "                                                discriminator.trainable_variables))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbfWmUSuTT0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(train_ds, epochs, val_ds, save_checkpoint, epochs_already_trained, tumor_area, loss):\n",
        "\n",
        "    psnr_to_plot = tf.TensorArray(tf.float32, size=epochs+1)\n",
        "    mse_to_plot = tf.TensorArray(tf.float32, size=epochs+1)\n",
        "    ssim_to_plot = tf.TensorArray(tf.float32, size=epochs+1) \n",
        "    if tumor_area:\n",
        "        psnr_to_plot_tumor = tf.TensorArray(tf.float32, size=epochs+1)\n",
        "        mse_to_plot_tumor = tf.TensorArray(tf.float32, size=epochs+1)\n",
        "        ssim_to_plot_tumor = tf.TensorArray(tf.float32, size=epochs+1) \n",
        "    if loss:\n",
        "        g_loss_to_plot = tf.TensorArray(tf.float32, size=epochs+1)\n",
        "        d_loss_to_plot = tf.TensorArray(tf.float32, size=epochs+1)\n",
        "    for epoch in range(epochs):\n",
        "    \n",
        "        if epoch == 0:\n",
        "            ########################### ALL THIS BLOCK IS TO VALIDATE THE RESULTS (and plot them) ##########\n",
        "            if tf.equal(tumor_area, True) and tf.equal(loss, False):\n",
        "                psnr, mse, ssim, psnr_tumor, mse_tumor, ssim_tumor = evaluate_GAN(generator, val_ds, 'validation', tumor_area, loss)\n",
        "            elif tf.equal(tumor_area, False) and tf.equal(loss, True):\n",
        "                psnr, mse, ssim, g_loss, d_loss = evaluate_GAN(generator, val_ds, 'validation', tumor_area, loss)\n",
        "            elif tf.equal(tumor_area, True) and tf.equal(loss, True):\n",
        "                psnr, mse, ssim, psnr_tumor, mse_tumor, ssim_tumor, g_loss, d_loss = evaluate_GAN(generator, val_ds, 'validation', tumor_area, loss)\n",
        "            else: \n",
        "                psnr, mse, ssim = evaluate_GAN(generator, val_ds, 'validation', tumor_area, loss)\n",
        "            psnr_to_plot = psnr_to_plot.write(epoch, psnr)\n",
        "            mse_to_plot = mse_to_plot.write(epoch, mse)\n",
        "            ssim_to_plot = ssim_to_plot.write(epoch, ssim)\n",
        "            if tumor_area:\n",
        "                psnr_to_plot_tumor = psnr_to_plot_tumor.write(epoch, psnr_tumor)\n",
        "                mse_to_plot_tumor = mse_to_plot_tumor.write(epoch, mse_tumor)\n",
        "                ssim_to_plot_tumor = ssim_to_plot_tumor.write(epoch, ssim_tumor)\n",
        "            if loss:\n",
        "                g_loss_to_plot = g_loss_to_plot.write(epoch, g_loss)\n",
        "                d_loss_to_plot = d_loss_to_plot.write(epoch, d_loss)\n",
        "            ################################################################################################\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        print()\n",
        "        print(\"Epoch: \", epoch + 1 + epochs_already_trained)     # I start with epoch 1: the first iteration (not epoch 0)\n",
        "\n",
        "        # Train\n",
        "        n = 0\n",
        "        \n",
        "        for idx, (t1, t2, t1c, t2flair, segmentation, patient) in train_ds.enumerate():\n",
        "            n += 1\n",
        "            print('.', end='')\n",
        "            if (n+1) % 100 == 0:\n",
        "                print()\n",
        "                print(n+1)\n",
        "            # This is the actual TRAINING STEP\n",
        "            train_step(t1, t2, t1c, t2flair)\n",
        "        ########################### ALL THIS BLOCK IS TO VALIDATE THE RESULTS (and plot them) ##########\n",
        "        if tf.equal(tumor_area, True) and tf.equal(loss, False):\n",
        "            psnr, mse, ssim, psnr_tumor, mse_tumor, ssim_tumor = evaluate_GAN(generator, val_ds, 'validation', tumor_area, loss)\n",
        "        elif tf.equal(tumor_area, False) and tf.equal(loss, True):\n",
        "            psnr, mse, ssim, g_loss, d_loss = evaluate_GAN(generator, val_ds, 'validation', tumor_area, loss)\n",
        "        elif tf.equal(tumor_area, True) and tf.equal(loss, True):\n",
        "            psnr, mse, ssim, psnr_tumor, mse_tumor, ssim_tumor, g_loss, d_loss = evaluate_GAN(generator, val_ds, 'validation', tumor_area, loss)\n",
        "        else: \n",
        "            psnr, mse, ssim = evaluate_GAN(generator, val_ds, 'validation', tumor_area, loss)\n",
        "        psnr_to_plot = psnr_to_plot.write(epoch+1, psnr)\n",
        "        mse_to_plot = mse_to_plot.write(epoch+1, mse)\n",
        "        ssim_to_plot = ssim_to_plot.write(epoch+1, ssim)\n",
        "        if tumor_area:\n",
        "            psnr_to_plot_tumor = psnr_to_plot_tumor.write(epoch+1, psnr_tumor)\n",
        "            mse_to_plot_tumor = mse_to_plot_tumor.write(epoch+1, mse_tumor)\n",
        "            ssim_to_plot_tumor = ssim_to_plot_tumor.write(epoch+1, ssim_tumor)\n",
        "        if loss:\n",
        "            g_loss_to_plot = g_loss_to_plot.write(epoch+1, g_loss)\n",
        "            d_loss_to_plot = d_loss_to_plot.write(epoch+1, d_loss)\n",
        "            ################################################################################################\n",
        "        print()\n",
        "\n",
        "        if (epoch + 1) % 5 == 0 and save_checkpoint:\n",
        "            ckpt_save_path = ckpt_manager.save()\n",
        "            print ('Saving checkpoint for epoch {} at {}'.format(epoch+ 1 + epochs_already_trained,\n",
        "                                                         ckpt_save_path))\n",
        "\n",
        "        print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1 + epochs_already_trained,\n",
        "                                                            time.time()-start))\n",
        "    if save_checkpoint:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving {} checkpoint for epoch {} at {}'.format(\"FINAL\", epoch+ 1 + epochs_already_trained,\n",
        "                                                         ckpt_save_path))\n",
        "        \n",
        "    psnr_to_plot = psnr_to_plot.stack()\n",
        "    mse_to_plot = mse_to_plot.stack()\n",
        "    ssim_to_plot = ssim_to_plot.stack()\n",
        "    if tumor_area:\n",
        "        psnr_to_plot_tumor = psnr_to_plot_tumor.stack()\n",
        "        mse_to_plot_tumor = mse_to_plot_tumor.stack()\n",
        "        ssim_to_plot_tumor = ssim_to_plot_tumor.stack()\n",
        "    if loss:\n",
        "        g_loss_to_plot = g_loss_to_plot.stack()\n",
        "        d_loss_to_plot = d_loss_to_plot.stack()\n",
        "        \n",
        "    if tf.equal(loss, True) and tf.equal(tumor_area, False):\n",
        "        return psnr_to_plot, mse_to_plot, ssim_to_plot, g_loss_to_plot, d_loss_to_plot\n",
        "    elif tf.equal(loss, False) and tf.equal(tumor_area, True):\n",
        "        return psnr_to_plot, mse_to_plot, ssim_to_plot, psnr_to_plot_tumor, mse_to_plot_tumor, ssim_to_plot_tumor\n",
        "    elif tf.equal(loss, True) and tf.equal(tumor_area, True):\n",
        "        return psnr_to_plot, mse_to_plot, ssim_to_plot, psnr_to_plot_tumor, mse_to_plot_tumor, ssim_to_plot_tumor, g_loss_to_plot, d_loss_to_plot\n",
        "    return psnr_to_plot, mse_to_plot, ssim_to_plot\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sppD3K8Lgy51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q pyyaml h5py  # Required to save models in HDF5 format"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V6bRxa2ykFs",
        "colab_type": "text"
      },
      "source": [
        "## Fit of the model (at the end it will also save the models)\n",
        "\n",
        "I save the models (I would need to save only the generator to be precise) in the same cell of the fitting because Google Colab, with long-run execution, doesn't allow to execute other cells after the one where I call 'fit'\n",
        "\n",
        "Suggestion: train for around 35 epochs. Long-run are discouraged since Google Colab will crash.\n",
        "Last time it arrived until the 43th epoch (409 seconds each).\n",
        "So MAX TRAINING TIME ALLOWED: almost 5 hours. -> after that GPU won't be available for 8 hours or so"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi55F4KTjNA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 35\n",
        "save_checkpoint = True         # want to save new checkpoints ? \n",
        "save_final_models = False       # want to save the model at the end of the training ? \n",
        "epochs_already_trained = 0      # this is just for the print. \n",
        "evaluate_tumor_quality = True   # compute & print metrics on tumor area ?\n",
        "print_loss = True               # compute & print the validation loss ?\n",
        "\n",
        "\n",
        "# Initialize the models, so that I don't have to restart the runtime again.\n",
        "\n",
        "if tf.equal(evaluate_tumor_quality, True) and tf.equal(print_loss, False):\n",
        "    psnr, mse, ssim, psnr_tumor, mse_tumor, ssim_tumor = fit(training, EPOCHS, validation, save_checkpoint, \n",
        "                                                    epochs_already_trained, evaluate_tumor_quality, print_loss)\n",
        "    plot_metrics(psnr, mse, ssim, psnr_tumor, mse_tumor, ssim_tumor)\n",
        "\n",
        "elif tf.equal(evaluate_tumor_quality, False) and tf.equal(print_loss, True):\n",
        "    psnr, mse, ssim, gen_loss, dis_loss = fit(training, EPOCHS, validation, save_checkpoint, \n",
        "                                                    epochs_already_trained, evaluate_tumor_quality, print_loss)\n",
        "    plot_metrics(psnr, mse, ssim)\n",
        "    plot_loss(gen_loss, dis_loss)\n",
        "\n",
        "elif tf.equal(evaluate_tumor_quality, True) and tf.equal(print_loss, True):\n",
        "    psnr, mse, ssim, psnr_tumor, mse_tumor, ssim_tumor, gen_loss, dis_loss = fit(training, EPOCHS, validation, save_checkpoint, \n",
        "                                                        epochs_already_trained, evaluate_tumor_quality, print_loss)\n",
        "    plot_metrics(psnr, mse, ssim, psnr_tumor, mse_tumor, ssim_tumor)\n",
        "    plot_loss(gen_loss, dis_loss)\n",
        "else:\n",
        "    psnr, mse, ssim = fit(training, EPOCHS, validation, save_checkpoint, \n",
        "                                                      epochs_already_trained, evaluate_tumor_quality, print_loss)\n",
        "    plot_metrics(psnr, mse, ssim)\n",
        "\n",
        "# at the end, I'll save the models (I would need only the generator)\n",
        "# (it saves the entire model to a HDF5 file)\n",
        "\n",
        "if save_final_models:\n",
        "    generator.save('drive/My Drive/MRI-generation/gen_t2flair_mip2p.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stn7gepaFKxZ",
        "colab_type": "text"
      },
      "source": [
        "##Evaluating the final model with the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qyqcc8Lyh_4_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's check the scores of this model.\n",
        "\n",
        "evaluate_GAN(generator, testing, 'test', True, True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
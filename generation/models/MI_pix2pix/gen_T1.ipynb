{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "gen_T1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPDCzCA5c50G",
        "colab_type": "text"
      },
      "source": [
        "## Import libraries and mount the drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngnfgyxdgvDv",
        "colab_type": "code",
        "outputId": "89221e84-6a9c-4b90-a100-b5c67ee0c9ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "try:\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from IPython import display\n",
        "import time\n",
        "import math\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-rc3\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g2TYHyiV-t_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('./drive/My Drive/Master_thesis/generation')\n",
        "from dataset_helpers import load_dataset\n",
        "from evaluation_metrics import compute_psnr, compute_ssim, compute_mse, compute_mse_tumor, compute_psnr_tumor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfK94uPI--2S",
        "colab_type": "text"
      },
      "source": [
        "## Loading train, validation and test sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GvRByzzuUfst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_datasets():\n",
        "    validation = load_dataset(prefix_path + 'brats2015_validation_crop_mri.tfrecords', ['MR_T1', 'MR_T1c', 'MR_T2', 'MR_Flair'], batch_size=BATCH_SIZE, shuffle=False)\n",
        "    training = load_dataset(prefix_path + 'brats2015_training_crop_mri.tfrecords', ['MR_T1', 'MR_T1c', 'MR_T2', 'MR_Flair'], batch_size=BATCH_SIZE, shuffle=True)\n",
        "    testing = load_dataset(prefix_path + 'brats2015_testing_crop_mri.tfrecords', ['MR_T1', 'MR_T1c', 'MR_T2', 'MR_Flair'], batch_size=BATCH_SIZE, shuffle=True)\n",
        "    return training, validation , testing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5la9-anCd8-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "prefix_path = './drive/My Drive/Master_thesis/datasets/'\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "training, validation, testing = load_datasets()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bEOWtEya_HY6",
        "colab_type": "text"
      },
      "source": [
        "## Load some useful functions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILemx0Oz3ZBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resize_with_crop(*args): # possibile arguments: input, gt, prediction and maybe the segmentation\n",
        "\n",
        "    image0 = tf.image.resize_with_crop_or_pad(args[0], 155, 194)\n",
        "    image1 = tf.image.resize_with_crop_or_pad(args[1], 155, 194)\n",
        "    image2 = tf.image.resize_with_crop_or_pad(args[2], 155, 194)\n",
        "    image3 = tf.image.resize_with_crop_or_pad(args[3], 155, 194)\n",
        "    if len(args) == 5:      # crop also the segmentation, if is given as additional argument\n",
        "        image4 = tf.image.resize_with_crop_or_pad(args[4], 155, 194)\n",
        "        return image0, image1, image2, image3, image4\n",
        "    if len(args) == 6:\n",
        "        image4 = tf.image.resize_with_crop_or_pad(args[4], 155, 194)\n",
        "        image5 = tf.image.resize_with_crop_or_pad(args[5], 155, 194)\n",
        "        return image0, image1, image2, image3, image4, image5\n",
        "    return image0, image1, image2, image3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kX5bu_es7zec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def retrieve_tumor_area(ground_truth, prediction, segmentation):\n",
        "    ground_truth_np = ground_truth.numpy()\n",
        "    segmentation_np = segmentation.numpy()\n",
        "    prediction_np = prediction.numpy()\n",
        "\n",
        "    # I want to remove all the pixels not relevant wrt the tumor area. \n",
        "    idx = (segmentation_np==0)      \n",
        "    ground_truth_np[idx] = segmentation_np[idx]\n",
        "    prediction_np[idx] = segmentation_np[idx]\n",
        "\n",
        "    return ground_truth_np, prediction_np      # Now the images are ready to be evaluated"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eS4RxJF_zVW",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svxcGvwIW_Di",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_GAN(model, dataset, set_type, evaluate_tumor_area=False, print_loss=False):\n",
        "    num_batches = 0                 # CAREFUL when batch_size is modified!!!\n",
        "    if set_type == 'test':\n",
        "        num_batches = 112           # in the test set, with batch 32, there are 112 elements.\n",
        "    elif set_type == 'validation':\n",
        "        num_batches = 108\n",
        "    elif set_type == 'train':\n",
        "        num_batches = 876\n",
        "    \n",
        "    container_psnr = tf.TensorArray(tf.float32, size=num_batches)     \n",
        "    container_mse = tf.TensorArray(tf.float32, size=num_batches)\n",
        "    container_ssim = tf.TensorArray(tf.float32, size=num_batches)\n",
        "    \n",
        "    if evaluate_tumor_area:\n",
        "        container_psnr_tumor = tf.TensorArray(tf.float32, size=num_batches)     \n",
        "        container_mse_tumor = tf.TensorArray(tf.float32, size=num_batches)\n",
        "        container_ssim_tumor = tf.TensorArray(tf.float32, size=num_batches)\n",
        "    if print_loss:\n",
        "        container_gen = tf.TensorArray(tf.float32, size=num_batches)     \n",
        "        container_disc = tf.TensorArray(tf.float32, size=num_batches)\n",
        "    \n",
        "    for idx, (t1, t2, t1c, t2flair, segmentation, patient) in dataset.enumerate():\n",
        "        # preparing the input to the generator\n",
        "        ground_truth = t1\n",
        "        input_tensor = tf.concat([t2, t1c, t2flair], 3)\n",
        "\n",
        "        # generate the prediction            \n",
        "        prediction = model(input_tensor, training=False)\n",
        "        \n",
        "        t2_cr, t1c_cr, t2flair_cr, gt_cr, pred_cr, segmentation_cr = resize_with_crop(t2, t1c, t2flair, ground_truth, prediction, segmentation)\n",
        "        \n",
        "        if evaluate_tumor_area:\n",
        "            prediction_normalized = mean_normalize(pred_cr)\n",
        "            ground_truth_normalized = mean_normalize(gt_cr) \n",
        "            ground_truth_masked, prediction_masked = retrieve_tumor_area(ground_truth_normalized, prediction_normalized, segmentation_cr)\n",
        "        \n",
        "        # PLOT ALWAYS THE SAME IMAGES, SO THAT IT'S EASIER TO PERCEIVE DIFFERENCES IN THE PREDICTIONS BETWEEN EPOCHS.\n",
        "        if (idx == 2 or idx == 25 or idx == 45 or idx == 50 or idx == 83) and evaluate_tumor_area:\n",
        "                plot_images(t2_cr, t1c_cr, t2flair_cr, gt_cr, pred_cr, ground_truth_masked, prediction_masked)\n",
        "        elif idx == 2 or idx == 25 or idx == 45 or idx == 50 or idx == 83:\n",
        "                plot_images(t2_cr, t1c_cr, t2flair_cr, gt_cr, pred_cr)\n",
        "\n",
        "        # normalize the prediction and the ground_truth\n",
        "        prediction_normalized_cr = mean_normalize(pred_cr)\n",
        "        ground_truth_normalized_cr = mean_normalize(gt_cr)\n",
        "\n",
        "        # compute the metrics of similarity\n",
        "        mean, std, psnr = compute_psnr(ground_truth_normalized_cr, prediction_normalized_cr)\n",
        "        container_psnr = container_psnr.write(idx, psnr)\n",
        "        mean, std, mse = compute_mse(ground_truth_normalized_cr, prediction_normalized_cr)\n",
        "        container_mse = container_mse.write(idx, mse)\n",
        "        mean, std, ssim = compute_ssim(ground_truth_normalized_cr, prediction_normalized_cr)\n",
        "        container_ssim = container_ssim.write(idx, ssim)\n",
        "\n",
        "        if evaluate_tumor_area:\n",
        "            mean, std, psnr = compute_psnr_tumor(ground_truth_masked, prediction_masked)\n",
        "            container_psnr_tumor = container_psnr_tumor.write(idx, psnr)\n",
        "            mean, std, mse = compute_mse_tumor(ground_truth_masked, prediction_masked)\n",
        "            container_mse_tumor = container_mse_tumor.write(idx, mse)\n",
        "            mean, std, ssim = compute_ssim(ground_truth_masked, prediction_masked)\n",
        "            container_ssim_tumor = container_ssim_tumor.write(idx, ssim)\n",
        "\n",
        "        # COMPUTES VALIDATION LOSSES FOR THE GENERATOR AND THE DISCRIMINATOR\n",
        "        if print_loss:\n",
        "            ####################################\n",
        "            disc_real_output = discriminator([input_tensor, ground_truth], training=True)\n",
        "                \n",
        "            # showing to D a batch fake images of T2\n",
        "            disc_generated_output = discriminator([input_tensor, prediction], training=True)\n",
        "\n",
        "            gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, prediction, ground_truth)\n",
        "            \n",
        "            disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "            container_gen = container_gen.write(idx, gen_total_loss)\n",
        "            container_disc = container_disc.write(idx, disc_loss)\n",
        "            ###############################\n",
        "\n",
        "    container_psnr = container_psnr.stack()\n",
        "    container_mse = container_mse.stack()\n",
        "    container_ssim = container_ssim.stack()\n",
        "    mean_psnr = tf.reduce_mean(tf.boolean_mask((container_psnr), tf.math.is_finite(container_psnr)))\n",
        "    std_psnr = tf.math.reduce_std(tf.boolean_mask((container_psnr), tf.math.is_finite(container_psnr)))\n",
        "    mean_mse = tf.reduce_mean(tf.boolean_mask((container_mse), tf.math.is_finite(container_mse)))\n",
        "    std_mse = tf.math.reduce_std(tf.boolean_mask((container_mse), tf.math.is_finite(container_mse)))\n",
        "    mean_ssim = tf.reduce_mean(tf.boolean_mask((container_ssim), tf.math.is_finite(container_ssim)))\n",
        "    std_ssim = tf.math.reduce_std(tf.boolean_mask((container_ssim), tf.math.is_finite(container_ssim)))\n",
        "\n",
        "    print(\"PSNR on {} set: {} ± {}\".format(set_type, (f'{mean_psnr:.4f}'), (f'{std_psnr:.4f}')))\n",
        "    print(\"MSE on {} set: {} ± {}\".format(set_type, (f'{mean_mse:.4f}'), (f'{std_mse:.4f}')))\n",
        "    print(\"SSIM on {} set: {} ± {}\".format(set_type, (f'{mean_ssim:.4f}'), (f'{std_ssim:.4f}')))\n",
        "\n",
        "    if evaluate_tumor_area:\n",
        "        container_psnr_tumor = container_psnr_tumor.stack()\n",
        "        container_mse_tumor = container_mse_tumor.stack()\n",
        "        container_ssim_tumor = container_ssim_tumor.stack()\n",
        "        \n",
        "        threshold = 228 # I don't consider values with PSNR > 228, because it means we are looking basically at black images.\n",
        "        container_psnr_tumor = tf.boolean_mask((container_psnr_tumor), tf.math.is_finite(container_psnr_tumor))\n",
        "        container_psnr_tumor = container_psnr_tumor[container_psnr_tumor < threshold]\n",
        "\n",
        "        mean_psnr_tumor = tf.reduce_mean(container_psnr_tumor)\n",
        "        std_psnr_tumor = tf.math.reduce_std(container_psnr_tumor)\n",
        "        mean_mse_tumor = tf.reduce_mean(tf.boolean_mask((container_mse_tumor), tf.math.is_finite(container_mse_tumor)))\n",
        "        std_mse_tumor = tf.math.reduce_std(tf.boolean_mask((container_mse_tumor), tf.math.is_finite(container_mse_tumor)))\n",
        "        mean_ssim_tumor = tf.reduce_mean(tf.boolean_mask((container_ssim_tumor), tf.math.is_finite(container_ssim_tumor)))\n",
        "        std_ssim_tumor = tf.math.reduce_std(tf.boolean_mask((container_ssim_tumor), tf.math.is_finite(container_ssim_tumor)))\n",
        "        print()\n",
        "        print(\"PSNR wrt tumor area on {} set: {} ± {}\".format(set_type, (f'{mean_psnr_tumor:.4f}'), (f'{std_psnr_tumor:.4f}')))\n",
        "        print(\"MSE wrt tumor area on {} set: {} ± {}\".format(set_type, (f'{mean_mse_tumor:.4f}'), (f'{std_mse_tumor:.4f}')))\n",
        "        print(\"SSIM wrt tumor area on {} set: {} ± {}\".format(set_type, (f'{mean_ssim_tumor:.4f}'), (f'{std_ssim_tumor:.4f}')))\n",
        "    if print_loss:\n",
        "        container_disc = container_disc.stack()\n",
        "        container_gen = container_gen.stack()\n",
        "        mean_gen = tf.reduce_mean(tf.boolean_mask((container_gen), tf.math.is_finite(container_gen)))\n",
        "        std_gen = tf.math.reduce_std(tf.boolean_mask((container_gen), tf.math.is_finite(container_gen)))\n",
        "        mean_disc = tf.reduce_mean(tf.boolean_mask((container_disc), tf.math.is_finite(container_disc)))\n",
        "        std_disc = tf.math.reduce_std(tf.boolean_mask((container_disc), tf.math.is_finite(container_disc)))\n",
        "        print()\n",
        "        print(\"Generator loss on {} set: {} ± {}\".format(set_type, (f'{mean_gen:.4f}'), (f'{std_gen:.4f}')))\n",
        "        print(\"Discriminator loss on {} set: {} ± {}\".format(set_type, (f'{mean_disc:.4f}'), (f'{std_disc:.4f}')))\n",
        "        \n",
        "    if tf.equal(print_loss, True) and tf.equal(evaluate_tumor_area, False):\n",
        "        return mean_psnr, mean_mse, mean_ssim, mean_gen, mean_disc\n",
        "    elif tf.equal(print_loss, False) and tf.equal(evaluate_tumor_area, True):\n",
        "        return mean_psnr, mean_mse, mean_ssim, mean_psnr_tumor, mean_mse_tumor, mean_ssim_tumor\n",
        "    elif tf.equal(print_loss, True) and tf.equal(evaluate_tumor_area, True):\n",
        "        return mean_psnr, mean_mse, mean_ssim, mean_psnr_tumor, mean_mse_tumor, mean_ssim_tumor, mean_gen, mean_disc\n",
        "    return mean_psnr, mean_mse, mean_ssim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3WAnSHCwo_CT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_loss(g_l, d_l):        \n",
        "    f, (ax1) = plt.subplots(1, 1, figsize=(12, 6))\n",
        "    f.subplots_adjust(hspace=0.4)\n",
        "    max_epoch = g_l.shape[0]       # args[0] \n",
        "    epoch_list = list(range(1,max_epoch+1))\n",
        "    ax1.plot(epoch_list, g_l, label='Generator loss')\n",
        "    ax1.plot(epoch_list, d_l, label='Discriminator Loss')\n",
        "    ax1.set_xticks(np.arange(1, max_epoch, 5))\n",
        "    ax1.set_ylabel('Loss Value')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_title('LOSS on validation set')\n",
        "    ax1.set_ylim([0.0, 12.0])\n",
        "    l1 = ax1.legend(loc=\"best\")\n",
        "    \n",
        "def plot_metrics(*args):        # arguments will be, in order: psnr, mse, ssim, psnr_tumor*, mse_tumor*, ssim_tumor*\n",
        "    tumor_area = False          # arguments with * are optional\n",
        "    if len(args) == 6:\n",
        "        tumor_area = True\n",
        "\n",
        "    f, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 20))\n",
        "    f.subplots_adjust(hspace=0.4)\n",
        "\n",
        "    max_epoch = (args[0]).shape[0]       # args[0] \n",
        "\n",
        "    epoch_list = list(range(1,max_epoch+1))\n",
        "    ax1.plot(epoch_list, args[0], label='PSNR')\n",
        "    if tumor_area: \n",
        "        ax1.plot(epoch_list, args[3], label='PSNR on tumor area')\n",
        "    ax1.set_xticks(np.arange(1, max_epoch, 5))\n",
        "    ax1.set_ylabel('PSNR Value')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_title('PSNR')\n",
        "    ax1.set_ylim([9, 35])\n",
        "    l1 = ax1.legend(loc=\"best\")\n",
        "\n",
        "    ax2.plot(epoch_list, args[1], label='MSE')\n",
        "    if tumor_area: \n",
        "        ax2.plot(epoch_list, args[4], label='MSE on tumor area')\n",
        "    ax2.set_xticks(np.arange(1, max_epoch, 5))\n",
        "    ax2.set_ylabel('MSE Value')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_title('MSE')\n",
        "    ax2.set_ylim([0.0, 0.10])\n",
        "    l2 = ax2.legend(loc=\"best\")\n",
        "\n",
        "    ax3.plot(epoch_list, args[2], label='SSIM')\n",
        "    if tumor_area: \n",
        "        ax3.plot(epoch_list, args[5], label='SSIM on tumor area')\n",
        "    ax3.set_xticks(np.arange(1, max_epoch, 5))\n",
        "    ax3.set_ylabel('SSIM Value')\n",
        "    ax3.set_xlabel('Epoch')\n",
        "    ax3.set_title('SSIM')\n",
        "    ax3.set_ylim([0.0, 1.0])\n",
        "    l3 = ax3.legend(loc=\"best\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioO19OjSm0Y3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_images(*args): \n",
        "\n",
        "    if len(args) == 5:\n",
        "        display_list = [args[0], args[1], args[2], args[3], args[4]]   # input, gt, prediction, gt masked, pred masked\n",
        "        title = ['T2', 'T1c', 't2flair', 'T1 - ground Truth', 'T1 - Predicted Image',]\n",
        "        figsize = (14, 7)\n",
        "    elif len(args) == 3:\n",
        "        display_list = [args[0], args[1], args[2]]   # input, gt and prediction\n",
        "        title = ['Input Image', 'Ground Truth', 'Predicted Image']\n",
        "        figsize = (8, 4)\n",
        "    elif len(args) == 7:\n",
        "        display_list = [args[0], args[1], args[2], args[3], args[4], args[5], args[6]]   # input, gt and prediction\n",
        "        title = ['T2', 'T1c', 'T2flair', 'T1 - ground Truth', 'T1 - Predicted Image', 'GT Tumor', 'Pred Tumor']\n",
        "        figsize = (22, 6)\n",
        "    \n",
        "    plt.figure(1 , figsize)\n",
        "    n = 0\n",
        "    for i in range(len(args)):    # batch size is different from 10, but let's show just 10 images.\n",
        "        n += 1\n",
        "        plt.subplot(1, len(args), n).title.set_text(title[i])\n",
        "        plt.imshow(tf.squeeze(display_list[i][0]), cmap='bone')\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98iV1zqoex77",
        "colab_type": "text"
      },
      "source": [
        "## Mean normalization\n",
        "\n",
        "![alt text](https://wikimedia.org/api/rest_v1/media/math/render/svg/5c591a0eeba163a12f69f937adbae5886d6273db)\n",
        "\n",
        "In the paper they say: \"Each patient scan is normalized by dividing each sequence by its mean intensity value. \"\n",
        "But the formula is taken from a lecture from Andrew Ng, where he defines the Mean normalization as in the formula above. (resource: https://www.youtube.com/watch?v=e1nTgoDI_m8)\n",
        "\n",
        "See also: https://stats.stackexchange.com/questions/138046/normalizations-dividing-by-mean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BURIoajNRLPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mean_rescale(x, xmin, xmax):\n",
        "    mean = tf.reduce_mean(x)\n",
        "    return ((x-mean)/(xmax-xmin)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dT_FH4yRI4j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def mean_normalize(image):\n",
        "    image_normalized = tf.TensorArray(tf.float32, size=BATCH_SIZE)\n",
        "    for i in range(BATCH_SIZE):\n",
        "        # rescaling each image in the batch\n",
        "        max_value = tf.math.reduce_max(image[i])\n",
        "        min_value = tf.math.reduce_min(image[i])\n",
        "        x = mean_rescale(image[i], min_value, max_value)\n",
        "        image_normalized = image_normalized.write(i, x)\n",
        "    image_normalized = image_normalized.stack()\n",
        "    return image_normalized\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NDDqVvqABDm",
        "colab_type": "text"
      },
      "source": [
        "## Discard black images from batch (put values to 'nan')\n",
        "\n",
        "This normalization is just to test the metrics and see if there is a big difference in normalizing the prediction and the gt.\n",
        "This method is used to normalize (and so put to 'nan') only the black images, while the other images of the batch are kept with the original values. This allows me to discard the black images in the computation of the metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVFWdUV2-3-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def black_images_to_nan(image): \n",
        "    image_normalized = tf.TensorArray(tf.float32, size=BATCH_SIZE)\n",
        "    for i in range(BATCH_SIZE):\n",
        "        # rescaling each image in the batch\n",
        "        max_value = tf.math.reduce_max(image[i])\n",
        "        min_value = tf.math.reduce_min(image[i])\n",
        "\n",
        "        # if the max = min most likely it's a black image (or an image without any important information)\n",
        "        if tf.math.equal(max_value, min_value):        \n",
        "            x = mean_rescale(image[i], min_value, max_value)\n",
        "            image_normalized = image_normalized.write(i, x)\n",
        "        else:\n",
        "            image_normalized = image_normalized.write(i, image[i])\n",
        "    image_normalized = image_normalized.stack()\n",
        "    return image_normalized"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-eVgrZJBpZ2",
        "colab_type": "text"
      },
      "source": [
        "## Defining models and optimizers - *G & D*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9O8q36ZDLDyy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "OUTPUT_CHANNELS = 4\n",
        "LAMBDA = 100\n",
        "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Z495D1VkowP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def downsample(filters, size, apply_batchnorm=True):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(\n",
        "      tf.keras.layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                             kernel_initializer=initializer, use_bias=False))\n",
        "\n",
        "  if apply_batchnorm:\n",
        "    result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  result.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeT7NojAZWm8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def upsample(filters, size, apply_dropout=False):\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  result = tf.keras.Sequential()\n",
        "  result.add(\n",
        "    tf.keras.layers.Conv2DTranspose(filters, size, strides=2,\n",
        "                                    padding='same',\n",
        "                                    kernel_initializer=initializer,\n",
        "                                    use_bias=False))\n",
        "\n",
        "  result.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "  if apply_dropout:\n",
        "      result.add(tf.keras.layers.Dropout(0.5))\n",
        "\n",
        "  result.add(tf.keras.layers.ReLU())\n",
        "\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uj3P879duquT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hint from the web: Unfortunately, UNet does not work with arbitrary input sizes. Try ResNet instead.\n",
        "\n",
        "def Generator():\n",
        "  inputs = tf.keras.layers.Input(shape=[256,256,3], batch_size=BATCH_SIZE)\n",
        "\n",
        "  down_stack = [\n",
        "    downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64)\n",
        "    downsample(128, 4), # (bs, 64, 64, 128)\n",
        "    downsample(256, 4), # (bs, 32, 32, 256)\n",
        "    downsample(512, 4), # (bs, 16, 16, 512)\n",
        "    downsample(512, 4), # (bs, 8, 8, 512)\n",
        "    downsample(512, 4), # (bs, 4, 4, 512)\n",
        "    downsample(512, 4), # (bs, 2, 2, 512)\n",
        "    downsample(512, 4), # (bs, 1, 1, 512)\n",
        "  ]\n",
        "  up_stack = [\n",
        "    upsample(512, 4, apply_dropout=True), # (bs, 2, 2, 1024)\n",
        "    upsample(512, 4, apply_dropout=True), # (bs, 4, 4, 1024)\n",
        "    upsample(512, 4, apply_dropout=True), # (bs, 8, 8, 1024)\n",
        "    upsample(512, 4), # (bs, 16, 16, 1024)\n",
        "    upsample(256, 4), # (bs, 32, 32, 512)\n",
        "    upsample(128, 4), # (bs, 64, 64, 256)\n",
        "    upsample(64, 4), # (bs, 128, 128, 128)\n",
        "  ]\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "  last = tf.keras.layers.Conv2DTranspose(1, 4, strides=2, padding='same',\n",
        "                                         kernel_initializer=initializer,activation='tanh') # (bs, 256, 256, 1)\n",
        "  x = inputs\n",
        "\n",
        "  # Downsampling through the model\n",
        "  skips = []\n",
        "  for down in down_stack:\n",
        "    x = down(x)\n",
        "    skips.append(x)\n",
        "\n",
        "  skips = reversed(skips[:-1])\n",
        "\n",
        "  # Upsampling and establishing the skip connections\n",
        "  for up, skip in zip(up_stack, skips):\n",
        "    x = up(x)\n",
        "    x = tf.keras.layers.Concatenate()([x, skip])\n",
        "\n",
        "  x = last(x)\n",
        "\n",
        "  return tf.keras.Model(inputs=inputs, outputs=x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4McnkYFXGlpG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generator_loss(disc_generated_output, gen_output, target):\n",
        "    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "    # mean absolute error\n",
        "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
        "    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n",
        "\n",
        "\n",
        "    return total_gen_loss, gan_loss, l1_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHtWMOwiLYnl",
        "colab_type": "code",
        "outputId": "a508f043-606a-4813-849d-3e757460a0d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "generator = Generator()\n",
        "generator.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_10\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_6 (InputLayer)            [(32, 256, 256, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "sequential_90 (Sequential)      (32, 128, 128, 64)   3072        input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "sequential_91 (Sequential)      (32, 64, 64, 128)    131584      sequential_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sequential_92 (Sequential)      (32, 32, 32, 256)    525312      sequential_91[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sequential_93 (Sequential)      (32, 16, 16, 512)    2099200     sequential_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sequential_94 (Sequential)      (32, 8, 8, 512)      4196352     sequential_93[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sequential_95 (Sequential)      (32, 4, 4, 512)      4196352     sequential_94[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sequential_96 (Sequential)      (32, 2, 2, 512)      4196352     sequential_95[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sequential_97 (Sequential)      (32, 1, 1, 512)      4196352     sequential_96[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sequential_98 (Sequential)      (32, 2, 2, 512)      4196352     sequential_97[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_40 (Concatenate)    (32, 2, 2, 1024)     0           sequential_98[0][0]              \n",
            "                                                                 sequential_96[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sequential_99 (Sequential)      (32, 4, 4, 512)      8390656     concatenate_40[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_41 (Concatenate)    (32, 4, 4, 1024)     0           sequential_99[0][0]              \n",
            "                                                                 sequential_95[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sequential_100 (Sequential)     (32, 8, 8, 512)      8390656     concatenate_41[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_42 (Concatenate)    (32, 8, 8, 1024)     0           sequential_100[0][0]             \n",
            "                                                                 sequential_94[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sequential_101 (Sequential)     (32, 16, 16, 512)    8390656     concatenate_42[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_43 (Concatenate)    (32, 16, 16, 1024)   0           sequential_101[0][0]             \n",
            "                                                                 sequential_93[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sequential_102 (Sequential)     (32, 32, 32, 256)    4195328     concatenate_43[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_44 (Concatenate)    (32, 32, 32, 512)    0           sequential_102[0][0]             \n",
            "                                                                 sequential_92[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sequential_103 (Sequential)     (32, 64, 64, 128)    1049088     concatenate_44[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_45 (Concatenate)    (32, 64, 64, 256)    0           sequential_103[0][0]             \n",
            "                                                                 sequential_91[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "sequential_104 (Sequential)     (32, 128, 128, 64)   262400      concatenate_45[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_46 (Concatenate)    (32, 128, 128, 128)  0           sequential_104[0][0]             \n",
            "                                                                 sequential_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_transpose_47 (Conv2DTran (32, 256, 256, 1)    2049        concatenate_46[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 54,421,761\n",
            "Trainable params: 54,410,881\n",
            "Non-trainable params: 10,880\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f91GydsSW6xG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Discriminator():\n",
        "  initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "  inp = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image', batch_size=BATCH_SIZE)\n",
        "  tar = tf.keras.layers.Input(shape=[256, 256, 1], name='target_image', batch_size=BATCH_SIZE)\n",
        "\n",
        "  x = tf.keras.layers.concatenate([inp, tar]) # (bs, 256, 256, channels*2)\n",
        "\n",
        "  down1 = downsample(64, 4, False)(x) # (bs, 128, 128, 64)\n",
        "  down2 = downsample(128, 4)(down1) # (bs, 64, 64, 128)\n",
        "  down3 = downsample(256, 4)(down2) # (bs, 32, 32, 256)\n",
        "\n",
        "  zero_pad1 = tf.keras.layers.ZeroPadding2D()(down3) # (bs, 34, 34, 256)\n",
        "  conv = tf.keras.layers.Conv2D(512, 4, strides=1, kernel_initializer=initializer, use_bias=False)(zero_pad1) #(bs, 31, 31, 512)\n",
        "\n",
        "  batchnorm1 = tf.keras.layers.BatchNormalization()(conv)\n",
        "\n",
        "  leaky_relu = tf.keras.layers.LeakyReLU()(batchnorm1)\n",
        "\n",
        "  zero_pad2 = tf.keras.layers.ZeroPadding2D()(leaky_relu) # (bs, 33, 33, 512)\n",
        "\n",
        "  last = tf.keras.layers.Conv2D(1, 4, strides=1, kernel_initializer=initializer)(zero_pad2) # (bs, 30, 30, 1)\n",
        "  return tf.keras.Model(inputs=[inp, tar], outputs=last)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD4gNdGPMb7Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def discriminator_loss(disc_real_output, disc_generated_output):\n",
        "  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
        "\n",
        "  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
        "\n",
        "  total_disc_loss = real_loss + generated_loss\n",
        "\n",
        "  return total_disc_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT0V9-gpXBfn",
        "colab_type": "code",
        "outputId": "728ea1f9-e03d-4ccb-80c8-212e1bdcf7f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        }
      },
      "source": [
        "discriminator = Discriminator()\n",
        "discriminator.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_11\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_image (InputLayer)        [(32, 256, 256, 3)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "target_image (InputLayer)       [(32, 256, 256, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_47 (Concatenate)    (32, 256, 256, 4)    0           input_image[0][0]                \n",
            "                                                                 target_image[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "sequential_105 (Sequential)     (32, 128, 128, 64)   4096        concatenate_47[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "sequential_106 (Sequential)     (32, 64, 64, 128)    131584      sequential_105[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "sequential_107 (Sequential)     (32, 32, 32, 256)    525312      sequential_106[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_10 (ZeroPadding2 (32, 34, 34, 256)    0           sequential_107[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (32, 31, 31, 512)    2097152     zero_padding2d_10[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_101 (BatchN (32, 31, 31, 512)    2048        conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_71 (LeakyReLU)      (32, 31, 31, 512)    0           batch_normalization_101[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "zero_padding2d_11 (ZeroPadding2 (32, 33, 33, 512)    0           leaky_re_lu_71[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (32, 30, 30, 1)      8193        zero_padding2d_11[0][0]          \n",
            "==================================================================================================\n",
            "Total params: 2,768,385\n",
            "Trainable params: 2,766,593\n",
            "Non-trainable params: 1,792\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNBW-N3dsMWa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2B2IbpYM9G-v",
        "colab_type": "text"
      },
      "source": [
        "## Let's load now the generator, discriminator weights and optimizers status from the checkpoints\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIgHGlcZHdg7",
        "colab_type": "text"
      },
      "source": [
        "Uncomment this only to continue to train an already trained network. Otherwise it will be trained a new GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AN1i5bXlF8ss",
        "colab_type": "code",
        "outputId": "1e2c4d47-57cd-4506-b919-effe7c8cedb9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "\"\"\"\n",
        "print(\"Optimizers and models defined but not initialized? {}\".format(generator_optimizer.weights == []))\n",
        "\n",
        "checkpoint_path = \"./drive/My Drive/checkpoints/gen_tflair_v2\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                discriminator_optimizer=discriminator_optimizer,\n",
        "                                generator=generator,\n",
        "                                discriminator=discriminator)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "    print ('Latest checkpoint restored!!')\n",
        "\"\"\"\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nprint(\"Optimizers and models defined but not initialized? {}\".format(generator_optimizer.weights == []))\\n\\ncheckpoint_path = \"./drive/My Drive/checkpoints/gen_tflair_v2\"\\n\\nckpt = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\\n                                discriminator_optimizer=discriminator_optimizer,\\n                                generator=generator,\\n                                discriminator=discriminator)\\n\\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\\n\\n\\n# if a checkpoint exists, restore the latest checkpoint.\\n\\nif ckpt_manager.latest_checkpoint:\\n    ckpt.restore(ckpt_manager.latest_checkpoint)\\n    print (\\'Latest checkpoint restored!!\\')\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 187
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYaMDs5p-DwE",
        "colab_type": "text"
      },
      "source": [
        "Now the GAN is restored to the previous checkpoint and ready to be trained again !!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCGLZV21lSR2",
        "colab_type": "text"
      },
      "source": [
        "##**TRAIN STEP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrFHtXhmTMmp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(t1, t2, t1c, t2flair):\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "\n",
        "        ground_truth = t1\n",
        "        # INPUT IMPUTATION\n",
        "        input_tensor = tf.concat([t2, t1c, t2flair], 3)\n",
        "\n",
        "        # generate the prediction            \n",
        "        prediction = generator(input_tensor, training=False)\n",
        "        disc_real_output = discriminator([input_tensor, ground_truth], training=True)\n",
        "            \n",
        "        # showing to D a batch fake images of T2\n",
        "        disc_generated_output = discriminator([input_tensor, prediction], training=True)\n",
        "\n",
        "        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, prediction, ground_truth)\n",
        "        \n",
        "        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n",
        "\n",
        "    generator_gradients = gen_tape.gradient(gen_total_loss,\n",
        "                                            generator.trainable_variables)\n",
        "    discriminator_gradients = disc_tape.gradient(disc_loss,\n",
        "                                                discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(generator_gradients,\n",
        "                                            generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n",
        "                                                discriminator.trainable_variables))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbfWmUSuTT0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit(train_ds, epochs, val_ds, save_checkpoint, epochs_already_trained, tumor_area, loss):\n",
        "\n",
        "    psnr_to_plot = tf.TensorArray(tf.float32, size=epochs+1)\n",
        "    mse_to_plot = tf.TensorArray(tf.float32, size=epochs+1)\n",
        "    ssim_to_plot = tf.TensorArray(tf.float32, size=epochs+1) \n",
        "    if tumor_area:\n",
        "        psnr_to_plot_tumor = tf.TensorArray(tf.float32, size=epochs+1)\n",
        "        mse_to_plot_tumor = tf.TensorArray(tf.float32, size=epochs+1)\n",
        "        ssim_to_plot_tumor = tf.TensorArray(tf.float32, size=epochs+1) \n",
        "    if loss:\n",
        "        g_loss_to_plot = tf.TensorArray(tf.float32, size=epochs+1)\n",
        "        d_loss_to_plot = tf.TensorArray(tf.float32, size=epochs+1)\n",
        "    for epoch in range(epochs):\n",
        "    \n",
        "        if epoch == 0:\n",
        "            ########################### ALL THIS BLOCK IS TO VALIDATE THE RESULTS (and plot them) ##########\n",
        "            if tf.equal(tumor_area, True) and tf.equal(loss, False):\n",
        "                psnr, mse, ssim, psnr_tumor, mse_tumor, ssim_tumor = evaluate_GAN(generator, val_ds, 'validation', tumor_area, loss)\n",
        "            elif tf.equal(tumor_area, False) and tf.equal(loss, True):\n",
        "                psnr, mse, ssim, g_loss, d_loss = evaluate_GAN(generator, val_ds, 'validation', tumor_area, loss)\n",
        "            elif tf.equal(tumor_area, True) and tf.equal(loss, True):\n",
        "                psnr, mse, ssim, psnr_tumor, mse_tumor, ssim_tumor, g_loss, d_loss = evaluate_GAN(generator, val_ds, 'validation', tumor_area, loss)\n",
        "            else: \n",
        "                psnr, mse, ssim = evaluate_GAN(generator, val_ds, 'validation', tumor_area, loss)\n",
        "            psnr_to_plot = psnr_to_plot.write(epoch, psnr)\n",
        "            mse_to_plot = mse_to_plot.write(epoch, mse)\n",
        "            ssim_to_plot = ssim_to_plot.write(epoch, ssim)\n",
        "            if tumor_area:\n",
        "                psnr_to_plot_tumor = psnr_to_plot_tumor.write(epoch, psnr_tumor)\n",
        "                mse_to_plot_tumor = mse_to_plot_tumor.write(epoch, mse_tumor)\n",
        "                ssim_to_plot_tumor = ssim_to_plot_tumor.write(epoch, ssim_tumor)\n",
        "            if loss:\n",
        "                g_loss_to_plot = g_loss_to_plot.write(epoch, g_loss)\n",
        "                d_loss_to_plot = d_loss_to_plot.write(epoch, d_loss)\n",
        "            ################################################################################################\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        print()\n",
        "        print(\"Epoch: \", epoch + 1 + epochs_already_trained)     # I start with epoch 1: the first iteration (not epoch 0)\n",
        "\n",
        "        # Train\n",
        "        n = 0\n",
        "        \n",
        "        for idx, (t1, t2, t1c, t2flair, segmentation, patient) in train_ds.enumerate():\n",
        "            n += 1\n",
        "            print('.', end='')\n",
        "            if (n+1) % 100 == 0:\n",
        "                print()\n",
        "                print(n+1)\n",
        "            # This is the actual TRAINING STEP\n",
        "            train_step(t1, t2, t1c, t2flair)\n",
        "        ########################### ALL THIS BLOCK IS TO VALIDATE THE RESULTS (and plot them) ##########\n",
        "        if tf.equal(tumor_area, True) and tf.equal(loss, False):\n",
        "            psnr, mse, ssim, psnr_tumor, mse_tumor, ssim_tumor = evaluate_GAN(generator, val_ds, 'validation', tumor_area, loss)\n",
        "        elif tf.equal(tumor_area, False) and tf.equal(loss, True):\n",
        "            psnr, mse, ssim, g_loss, d_loss = evaluate_GAN(generator, val_ds, 'validation', tumor_area, loss)\n",
        "        elif tf.equal(tumor_area, True) and tf.equal(loss, True):\n",
        "            psnr, mse, ssim, psnr_tumor, mse_tumor, ssim_tumor, g_loss, d_loss = evaluate_GAN(generator, val_ds, 'validation', tumor_area, loss)\n",
        "        else: \n",
        "            psnr, mse, ssim = evaluate_GAN(generator, val_ds, 'validation', tumor_area, loss)\n",
        "        psnr_to_plot = psnr_to_plot.write(epoch+1, psnr)\n",
        "        mse_to_plot = mse_to_plot.write(epoch+1, mse)\n",
        "        ssim_to_plot = ssim_to_plot.write(epoch+1, ssim)\n",
        "        if tumor_area:\n",
        "            psnr_to_plot_tumor = psnr_to_plot_tumor.write(epoch+1, psnr_tumor)\n",
        "            mse_to_plot_tumor = mse_to_plot_tumor.write(epoch+1, mse_tumor)\n",
        "            ssim_to_plot_tumor = ssim_to_plot_tumor.write(epoch+1, ssim_tumor)\n",
        "        if loss:\n",
        "            g_loss_to_plot = g_loss_to_plot.write(epoch+1, g_loss)\n",
        "            d_loss_to_plot = d_loss_to_plot.write(epoch+1, d_loss)\n",
        "            ################################################################################################\n",
        "        print()\n",
        "\n",
        "        if (epoch + 1) % 5 == 0 and save_checkpoint:\n",
        "            ckpt_save_path = ckpt_manager.save()\n",
        "            print ('Saving checkpoint for epoch {} at {}'.format(epoch+ 1 + epochs_already_trained,\n",
        "                                                         ckpt_save_path))\n",
        "\n",
        "        print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1 + epochs_already_trained,\n",
        "                                                            time.time()-start))\n",
        "    if save_checkpoint:\n",
        "        ckpt_save_path = ckpt_manager.save()\n",
        "        print ('Saving {} checkpoint for epoch {} at {}'.format(\"FINAL\", epoch+ 1 + epochs_already_trained,\n",
        "                                                         ckpt_save_path))\n",
        "        \n",
        "    psnr_to_plot = psnr_to_plot.stack()\n",
        "    mse_to_plot = mse_to_plot.stack()\n",
        "    ssim_to_plot = ssim_to_plot.stack()\n",
        "    if tumor_area:\n",
        "        psnr_to_plot_tumor = psnr_to_plot_tumor.stack()\n",
        "        mse_to_plot_tumor = mse_to_plot_tumor.stack()\n",
        "        ssim_to_plot_tumor = ssim_to_plot_tumor.stack()\n",
        "    if loss:\n",
        "        g_loss_to_plot = g_loss_to_plot.stack()\n",
        "        d_loss_to_plot = d_loss_to_plot.stack()\n",
        "        \n",
        "    if tf.equal(loss, True) and tf.equal(tumor_area, False):\n",
        "        return psnr_to_plot, mse_to_plot, ssim_to_plot, g_loss_to_plot, d_loss_to_plot\n",
        "    elif tf.equal(loss, False) and tf.equal(tumor_area, True):\n",
        "        return psnr_to_plot, mse_to_plot, ssim_to_plot, psnr_to_plot_tumor, mse_to_plot_tumor, ssim_to_plot_tumor\n",
        "    elif tf.equal(loss, True) and tf.equal(tumor_area, True):\n",
        "        return psnr_to_plot, mse_to_plot, ssim_to_plot, psnr_to_plot_tumor, mse_to_plot_tumor, ssim_to_plot_tumor, g_loss_to_plot, d_loss_to_plot\n",
        "    return psnr_to_plot, mse_to_plot, ssim_to_plot\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sppD3K8Lgy51",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q pyyaml h5py  # Required to save models in HDF5 format"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6V6bRxa2ykFs",
        "colab_type": "text"
      },
      "source": [
        "## Fit of the model (at the end it will also save the models)\n",
        "\n",
        "I save the models (I would need to save only the generator to be precise) in the same cell of the fitting because Google Colab, with long-run execution, doesn't allow to execute other cells after the one where I call 'fit'\n",
        "\n",
        "Suggestion: train for around 35 epochs. Long-run are discouraged since Google Colab will crash.\n",
        "Last time it arrived until the 43th epoch (409 seconds each).\n",
        "So MAX TRAINING TIME ALLOWED: almost 5 hours. -> after that GPU won't be available for 8 hours or so"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi55F4KTjNA0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 35\n",
        "save_checkpoint = True         # want to save new checkpoints ? \n",
        "save_final_models = False       # want to save the model at the end of the training ? \n",
        "epochs_already_trained = 0      # this is just for the print. \n",
        "evaluate_tumor_quality = True   # compute & print metrics on tumor area ?\n",
        "print_loss = True               # compute & print the validation loss ?\n",
        "\n",
        "\n",
        "# Initialize the models, so that I don't have to restart the runtime again.\n",
        "\n",
        "if tf.equal(evaluate_tumor_quality, True) and tf.equal(print_loss, False):\n",
        "    psnr, mse, ssim, psnr_tumor, mse_tumor, ssim_tumor = fit(training, EPOCHS, validation, save_checkpoint, \n",
        "                                                    epochs_already_trained, evaluate_tumor_quality, print_loss)\n",
        "    plot_metrics(psnr, mse, ssim, psnr_tumor, mse_tumor, ssim_tumor)\n",
        "\n",
        "elif tf.equal(evaluate_tumor_quality, False) and tf.equal(print_loss, True):\n",
        "    psnr, mse, ssim, gen_loss, dis_loss = fit(training, EPOCHS, validation, save_checkpoint, \n",
        "                                                    epochs_already_trained, evaluate_tumor_quality, print_loss)\n",
        "    plot_metrics(psnr, mse, ssim)\n",
        "    plot_loss(gen_loss, dis_loss)\n",
        "\n",
        "elif tf.equal(evaluate_tumor_quality, True) and tf.equal(print_loss, True):\n",
        "    psnr, mse, ssim, psnr_tumor, mse_tumor, ssim_tumor, gen_loss, dis_loss = fit(training, EPOCHS, validation, save_checkpoint, \n",
        "                                                        epochs_already_trained, evaluate_tumor_quality, print_loss)\n",
        "    plot_metrics(psnr, mse, ssim, psnr_tumor, mse_tumor, ssim_tumor)\n",
        "    plot_loss(gen_loss, dis_loss)\n",
        "else:\n",
        "    psnr, mse, ssim = fit(training, EPOCHS, validation, save_checkpoint, \n",
        "                                                      epochs_already_trained, evaluate_tumor_quality, print_loss)\n",
        "    plot_metrics(psnr, mse, ssim)\n",
        "\n",
        "# at the end, I'll save the models (I would need only the generator)\n",
        "# (it saves the entire model to a HDF5 file)\n",
        "\n",
        "if save_final_models:\n",
        "    generator.save('drive/My Drive/MRI-generation/gen_t2flair_mip2p.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "stn7gepaFKxZ",
        "colab_type": "text"
      },
      "source": [
        "##Evaluating the final model with the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qyqcc8Lyh_4_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's check the scores of this model.\n",
        "\n",
        "evaluate_GAN(generator, testing, 'test', True, True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}